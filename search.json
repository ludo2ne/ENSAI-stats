[
  {
    "objectID": "doc/tuto/stats/tests-statitiques.html",
    "href": "doc/tuto/stats/tests-statitiques.html",
    "title": "Tests Statistiques",
    "section": "",
    "text": "Le but du test est de rejeter \\(H_0\\) avec la plus petite erreur possible."
  },
  {
    "objectID": "doc/tuto/stats/tests-statitiques.html#types-de-tests",
    "href": "doc/tuto/stats/tests-statitiques.html#types-de-tests",
    "title": "Tests Statistiques",
    "section": "1 Types de tests",
    "text": "1 Types de tests\n\nselon hypoth√®ses\n\nsimple vs.¬†simple : \\(H_0 : \\theta = \\theta_0 \\  vs \\  H_1 : \\theta = \\theta_1\\)\nsimple vs.¬†multiple (bilat√©ral): \\(H_0 : \\theta = \\theta_0 \\  vs \\  H_1 : \\theta \\ne \\theta_0\\)\nmultiple vs.¬†simple (unilat√©ral): \\(H_0 : \\theta \\le \\theta_0 \\  vs \\  H_1 : \\theta &gt; \\theta_0\\)\n\nselon loi param√©trique ou non\nobjet test√© : moyenne, variance, ind√©pendance‚Ä¶"
  },
  {
    "objectID": "doc/tuto/stats/tests-statitiques.html#zone-de-rejet",
    "href": "doc/tuto/stats/tests-statitiques.html#zone-de-rejet",
    "title": "Tests Statistiques",
    "section": "2 Zone de rejet",
    "text": "2 Zone de rejet\n\n\\(T\\) : Stat de test \\(=f¬∞(echantillon)\\)\n\\(\\mathcal{R}\\) : Zone de rejet\n\nde faible probabilit√© pour T sachant \\(H_0\\)\nSi \\(t \\in \\mathcal{R}\\) : on rejette \\(H_0\\) (o√π t est la r√©alisation de T)\n\nRisque de 1ere esp√®ce : \\(\\alpha = \\mathbb{P}(T \\in \\mathcal{R} \\ | \\ H_0)\\)\n\n\\(\\alpha\\) (Niveau de test) = Probabilit√© de rejeter \\(H_0\\) alors qu‚Äôelle est vraie\nc‚Äôest l‚Äôerreur la plus grave qu‚Äôil faut surtout √©viter\nexemple : d√©clarer coupable un innocent\n\nRisque de 2e esp√®ce : \\(\\beta\\) tel que \\(\\beta = \\mathbb{P}(T \\in \\overline{\\mathcal{R}} \\ | \\ H_1)\\)\n\nProbabilit√© d‚Äôaccepter \\(H_0\\) alors qu‚Äôelle est fausse\nexemple : d√©clarer innocent un coupable\n\nPuissance d'un test : probabilit√© de rejeter \\(H_0\\) alors qu‚Äôelle est fausse\n\n\\(1 - \\beta = \\mathbb{P}(T \\in \\mathcal{R} \\ | \\ H_1)\\)\n\nUn test est sans biais si \\(1 - \\beta \\ge \\alpha\\) (Puissance \\(\\ge\\) Niveau de test)\n\\(T_1\\) est plus puissant que \\(T_2\\) si pour un m√™me niveau de test \\(\\alpha\\), \\(1 - \\beta_1 \\ge 1 - \\beta_2\\)"
  },
  {
    "objectID": "doc/tuto/stats/tests-statitiques.html#etude-dun-cas-concret",
    "href": "doc/tuto/stats/tests-statitiques.html#etude-dun-cas-concret",
    "title": "Tests Statistiques",
    "section": "3 Etude d‚Äôun cas concret",
    "text": "3 Etude d‚Äôun cas concret\nüéØ Objectif : nous souhaitons tester si une pi√®ce est non truqu√©e.\nNous utiliserons une loi de Bernoulli pour mod√©liser nos lancers, avec par exemple :\n\npile : 1\nface : 0\n\n\nset.seed(1986)\n\nn &lt;- 100     # nombre de lancers\np &lt;- 0.5     # Probabilit√© d'avoir un pile\n\n# Simulation de n lancers\nX &lt;- rbinom(n, 1, prob = p)\n\n# Quelques statistiques descriptives\ntable(X)\n\nX\n 0  1 \n49 51 \n\nprop.table(table(X))\n\nX\n   0    1 \n0.49 0.51 \n\nmean(X)\n\n[1] 0.51\n\nvar(X)     # p * (1-p)\n\n[1] 0.2524242\n\n\n\n3.1 LFGN\nLa Loi Forte des Grands Nombres nous dit que :\n\nplus nous augmentons le nombre de lancers\nplus la moyenne empirique converge vers l‚Äôesp√®rance\n\nCalculons les moyennes cumul√©es et tra√ßons l‚Äô√©volution lorsque le nombre de lancers augmente\n\n# Somme cumulative divis√©e par le nombre de lancers correspondants\nx_bar &lt;- cumsum(X) / seq_along(X)\n\ndf &lt;- data.frame(x = seq_along(x_bar), x_bar = x_bar)\n\n\n\nAutre m√©thode pour calculer x_bar\nx_bar &lt;- rep(NA, n)\nfor (i in 1:n) {\n  x_bar[i] &lt;- sum(X[1:i]) / i\n}\n\n\n\n\nCode ggplot\nlibrary(ggplot2)\n\nggplot(df, aes(x, x_bar)) +\n  geom_line(color = \"darkcyan\", \n            linewidth = 1.5) +\n  geom_hline(yintercept = p, \n             linetype = \"dashed\", \n             color = \"darkorange\") +\n  labs(x = \"Nombre de lancers\", \n       y = \"Moyenne\", \n       title = \"√âvolution de la moyenne selon le nombre de lancers de pi√®ce\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n3.2 TCL\nApplication du TCL :\n\nnous r√©p√®tons L fois l‚Äôexp√©rience de n lancers\n√† chaque fois, calculons la moyenne obtenue\npuis nous normalisons notre √©chantillon\nenfin, tra√ßons l‚Äôhistogramme des moyennes\n\n\nset.seed(1986)\n\nL &lt;- 1000\nn &lt;- 100\np &lt;- 0.5\n\nx_bar &lt;- numeric(L)\nfor (i in 1:L) {\n  X &lt;- rbinom(n, 1, prob = p)\n  x_bar[i] &lt;- mean(X)\n}\n\n# Normalisation\nY &lt;- sqrt(n) * (x_bar - p) / sqrt(p * (1 - p))\n\n\n\nCode ggplot\nlibrary(ggplot2)\n\nggplot(data.frame(Y), aes(x = Y)) +\n  geom_histogram(binwidth = 0.20, \n                 color = \"black\", \n                 fill = \"darkcyan\") +\n  labs(x = \"Valeurs normalis√©es\", \n       y = \"Fr√©quence\", \n       title = \"Histogramme des valeurs normalis√©es\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nLa distribution nous fait bien penser √† la loi normale centr√©e r√©duite.\n\n\nCode ggplot\ndensity_Y &lt;- density(Y)\n\n# Create data for standard normal density curve\nstd_norm_curve &lt;- data.frame(x = seq(-4, 4, length.out = 100),\n                             y = dnorm(seq(-4, 4, length.out = 100)))\n\n\nlegend_colors &lt;- c(\"Densit√© empirique\" = \"darkcyan\", \"Densit√© N(0,1)\" = \"darkorange\")\n\n# Create ggplot with density plot and standard normal density curve\nggplot() +\n  geom_line(data = data.frame(x = density_Y$x, \n                              y = density_Y$y), \n            aes(x, y, color = \"Densit√© empirique\"),\n            linewidth = 1.5) +\n  geom_line(data = std_norm_curve, \n            aes(x, y, color = \"Densit√© N(0,1)\"), \n            linetype = \"dashed\", \n            linewidth = 1) +\n  xlim(-4, 4) +\n  labs(x = \"Valeurs normalis√©es\", \n       y = \"Densit√©\", \n       title = \"Comparaison des densit√©s empiriques et th√©oriques\",\n       color = \"L√©gende\") + \n  scale_color_manual(values = legend_colors) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n3.3 Mon premier test\nSupposons que nous lan√ßons 100 fois une pi√®ce et que nous obtenons :\n\n59 pile (1)\n41 face (0)\n\n\nn &lt;- 100\nnb_pile &lt;- 59\nX &lt;- c(rep(1, nb_pile), rep(0, n - nb_pile))\ntable(X)\n\nX\n 0  1 \n41 59 \n\n\nOk, il y a un √©cart significatif entre le nombre de piles et de faces, mais est-ce que cela signifie que la pi√®ce est truqu√©e ?\nNous allons √©tablir un test pour voir si la moyenne empirique obtenue s‚Äô√©loigne trop de la moyenne th√©orique.\n\nSoit \\(H_0\\) : la pi√®ce n‚Äôest pas truqu√©e (p = 0.5)\nVs \\(H_1\\) : la pi√®ce est truqu√©e (p ‚â† 0.5)\n\nNous d√©finissons le niveau de risque \\(\\alpha\\).\nUn niveau de risque √† 0.05 signifie que l‚Äôon veut √™tre s√ªr √† 95 % de ne pas rejeter \\(H_0\\) alors qu‚Äôelle est vraie.\n\nalpha &lt;- 0.05\nx_bar &lt;- mean(X)\n\nSoit \\(T\\) la statistique de test\n\ncalcul√©e √† partir des donn√©es observ√©es,\nutilis√©e pour √©valuer la plausibilit√© de l‚Äôhypoth√®se nulle\n\nsert √† quantifier l‚Äô√©cart entre les donn√©es observ√©es et ce √† quoi on s‚Äôattendrait si l‚Äôhypoth√®se nulle √©tait vraie\n\n\n\nT &lt;- sqrt(n) * (x_bar - 0.5) / sqrt(p * (1 - p)); T\n\n[1] 1.8\n\n\nSous \\(H_0\\), \\(T\\) suit une loi Normale Centr√©e R√©duite :\n\nnous d√©finissons donc la zone de rejet\n\nla zone de rejet est la r√©gion o√π les valeurs observ√©es sont tellement extr√™mes que notre hypoth√®se de d√©part semble fausse\n\nnous rejetons \\(H_0\\) si T n‚Äôappartient pas √† l‚Äôintervalle suivant\n\n\nc(-1, 1) * qnorm(1 - alpha / 2)\n\n[1] -1.959964  1.959964\n\n\nEst-ce que l‚Äôon rejette \\(H_0\\) ?\nAutrement dit, est-ce que \\(T\\) appartient √† la zone de rejet ?\n\nT &lt; -qnorm(1 - alpha / 2) | T &gt; qnorm(1 - alpha / 2)\n\n[1] FALSE\n\n\nOn ne peut pas rejeter \\(H_0\\) au niveau de risque 0.05.\nRepr√©sentation graphique :\n\n\nCode ggplot\n# Donn√©es pour la courbe de densit√© normale\nx_values &lt;- seq(-4, 4, length.out = 100)\ny_values &lt;- dnorm(x_values)\n\n# Donn√©es pour les zones de rejet\nzone_rejet_gauche &lt;- seq(-4, qnorm(alpha/2), 0.01)\nzone_rejet_droite &lt;- seq(qnorm(1-alpha/2), 4, 0.01)\n\n# Cr√©ation du graphique avec ggplot2\nggplot() +\n  geom_line(data = data.frame(x = x_values, \n                              y = y_values), \n            aes(x, y), \n            color = \"darkcyan\", \n            linewidth = 1.5) +\n  geom_polygon(data = data.frame(x = c(zone_rejet_gauche, \n                                       zone_rejet_gauche[length(zone_rejet_gauche)]), \n                                 y = c(dnorm(zone_rejet_gauche), 0)), \n               aes(x, y), \n               fill = \"darkorange\") +\n  geom_polygon(data = data.frame(x = c(zone_rejet_droite, \n                                       zone_rejet_droite[length(zone_rejet_droite)]), \n                                 y = c(0, dnorm(zone_rejet_droite))), \n               aes(x, y), \n               fill = \"darkorange\") +\n  geom_rug(data = data.frame(T = T), \n           aes(x = T), \n           sides = \"b\", \n           color = \"blue\", \n           linewidth = 1) +\n  xlim(-4, 4) +\n  labs(x = \"x\", y = \"Densit√©\", title = \"Loi de T sous H0\") +\n  theme_minimal() +\n  scale_fill_manual(values = \"darkorange\", guide = FALSE) +\n  guides(color = \"none\") +\n  annotate(\"text\", x = 3, y = 0.1, label = \"Zones de rejet\", color = \"darkorange\") +\n  annotate(\"text\", x = T, y = 0.02, label = \"T\", color = \"blue\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nNous remarquons que \\(T\\) ne se trouve pas dans la zone de rejet\nSi \\(T\\) se trouvait dans la zone de rejet, nous aurions affirm√© qu‚Äôil est peu probable que \\(T\\) suive une \\(\\mathcal{N}(0, 1)\\)\n\nnous aurions donc rejet√© \\(H_0\\)\n\n\n\n\n3.4 p-valeur\n\nProbabilit√© d‚Äôobtenir une valeur aussi extr√™me que celle observ√©e\nplus grande valeur possible de \\(\\alpha\\) conduisant √† ne pas rejeter \\(H_0\\)\n\n\np_val &lt;- 2 * (1 - pnorm(T)) ; p_val\n\n[1] 0.07186064\n\n\nLa p-valeur vaut environs 11%. Cela signifie que si nous avions choisi un niveau de risque de :\n\n10% : nous ne rejettons pas \\(H_0\\) et consid√©rons que la pi√®ce n‚Äôest pas truqu√©e\n12% : nous rejettons \\(H_0\\)\n\n\n\n3.5 Test unilat√©ral\nJusqu‚Äô√† maintenant, nous √©tions dans le cas d‚Äôun test bilat√©ral.\nC‚Äôest √† dire que nous testions pour d√©terminer si la pi√®ce est truqu√©e, sans nous pr√©occuper si elle avantage le c√¥t√© pile ou le c√¥t√© face.\nMaintenant, imaginons que nous souhaitons tester si elle est truqu√©e c√¥t√© pile uniquement. Nous avons donc :\n\nSoit \\(H_0\\) : la pi√®ce n‚Äôest pas truqu√©e (p = 0.5)\nVs \\(H_1\\) : la pi√®ce est truqu√©e c√¥t√© pile (p &gt; 0.5)\n\nLa statistique de test \\(T\\) ne change pas.\nNous rejetons si p est trop grand, donc la zone de rejet est :\n\nc(qnorm(1 - alpha), Inf)\n\n[1] 1.644854      Inf\n\n\nEst-ce que l‚Äôon rejette \\(H_0\\) ?\n\nT &gt; qnorm(1 - alpha)\n\n[1] TRUE\n\n\nAu niveau de risque 0.05, nous rejetons \\(H_0\\) et donc la pi√®ce est truqu√©e c√¥t√© pile.\nRepr√©sentation graphique :\n\n\nCode ggplot\n# Donn√©es pour la courbe de densit√© normale\nx_values &lt;- seq(-4, 4, length.out = 100)\ny_values &lt;- dnorm(x_values)\n\n# Donn√©es pour les zones de rejet\nzone_rejet_droite &lt;- seq(qnorm(1-alpha), 4, 0.01)\n\n# Cr√©ation du graphique avec ggplot2\nggplot() +\n  geom_line(data = data.frame(x = x_values, \n                              y = y_values), \n            aes(x, y), \n            color = \"darkcyan\", \n            linewidth = 1.5) +\n  geom_polygon(data = data.frame(x = c(zone_rejet_droite, \n                                       zone_rejet_droite[length(zone_rejet_droite)]), \n                                 y = c(0, dnorm(zone_rejet_droite))), \n               aes(x, y), \n               fill = \"darkorange\") +\n  geom_rug(data = data.frame(T = T), \n           aes(x = T), \n           sides = \"b\", \n           color = \"blue\", \n           linewidth = 1) +\n  xlim(-4, 4) +\n  labs(x = \"x\", y = \"Densit√©\", title = \"Loi de T sous H0\") +\n  theme_minimal() +\n  scale_fill_manual(values = \"darkorange\", guide = FALSE) +\n  guides(color = \"none\") +\n  annotate(\"text\", x = 3, y = 0.1, label = \"Zones de rejet\", color = \"darkorange\") +\n  annotate(\"text\", x = T, y = 0.02, label = \"T\", color = \"blue\") +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "doc/tuto/proba/probabilit√©s.html",
    "href": "doc/tuto/proba/probabilit√©s.html",
    "title": "Probabilit√©",
    "section": "",
    "text": "\\[\\mathbb{P}(X \\leq x) = \\mathbb{E}(\\mathbb{1}_{X \\leq x})\\] \\[\\mathbb{V}(X) = \\mathbb{E}((X-\\mathbb{E}(X))^2)\\]"
  },
  {
    "objectID": "doc/tuto/proba/probabilit√©s.html#formules-de-base",
    "href": "doc/tuto/proba/probabilit√©s.html#formules-de-base",
    "title": "Probabilit√©",
    "section": "",
    "text": "\\[\\mathbb{P}(X \\leq x) = \\mathbb{E}(\\mathbb{1}_{X \\leq x})\\] \\[\\mathbb{V}(X) = \\mathbb{E}((X-\\mathbb{E}(X))^2)\\]"
  },
  {
    "objectID": "doc/tuto/proba/probabilit√©s.html#convergences",
    "href": "doc/tuto/proba/probabilit√©s.html#convergences",
    "title": "Probabilit√©",
    "section": "2 Convergences",
    "text": "2 Convergences\n\nConvergence presque surement\n\n\\(X_n \\underset{n \\to +\\infty}{\\overset{p.s} \\longrightarrow} X \\Longleftrightarrow \\forall \\omega \\in \\Omega, \\lim\\limits_{n \\rightarrow +\\infty} X_n(\\omega) = X(\\omega)\\)\n\nConvergence en probabilit√©\n\n\\(X_n \\underset{n \\to +\\infty}{\\overset{\\mathbb{P}} \\longrightarrow} X \\Longleftrightarrow \\forall \\epsilon, \\lim\\limits_{n \\rightarrow +\\infty} \\mathbb{P}(||X_n-X||&gt;\\epsilon) = 0\\)\n\nConvergence dans \\(\\mathbb{L}^p\\)\n\n\\(X_n \\underset{n \\to +\\infty}{\\overset{\\mathbb{L}^p} \\longrightarrow} X \\Longleftrightarrow \\lim\\limits_{n \\rightarrow +\\infty} ||X_n-X||_p = 0\\)\n\nConvergence en Loi\n\n\\(X_n \\underset{n \\to +\\infty}{\\overset{\\mathcal{L}} \\longrightarrow} X \\Longleftrightarrow \\lim\\limits_{n \\rightarrow +\\infty} \\mathbb{E}(h(X_n)) = \\mathbb{E}(h(X)), \\forall h\\) continue born√©e\n\n\nImplications de convergences :\n\n\\(p.s \\Rightarrow \\mathbb{P}\\)\n\\(\\mathbb{L}^p \\Rightarrow \\mathbb{P}\\)\n\\(\\mathbb{P} \\Rightarrow \\mathcal{L}\\)"
  },
  {
    "objectID": "doc/tuto/proba/probabilit√©s.html#loi-forte-des-grands-nombres",
    "href": "doc/tuto/proba/probabilit√©s.html#loi-forte-des-grands-nombres",
    "title": "Probabilit√©",
    "section": "3 Loi forte des Grands Nombres",
    "text": "3 Loi forte des Grands Nombres\n\n3.1 √ânonc√©\nSoit \\((X_n)_{n\\geq1}\\) une suite de variables al√©atoires ind√©pendantes et identiquement distribu√©es (iid) et int√©grables √† valeurs dans \\(\\mathbb{R}^d\\). Alors :\n\\[\\frac{1}{n} \\sum_{i=1}^{n} X_i \\underset{n \\to +\\infty}{\\overset{\\text{p.s.}}{\\longrightarrow}} \\mathbb{E}(X_1)\\]\nFormulation simple : la moyenne empirique tend vers l‚Äôesp√©rance.\n\n\n3.2 Exemple\n\nLan√ßons un d√© non truqu√© 500 fois et calculons la moyenne obtenue apr√©s chaque lancer\nPlus le nombre de lancers augmente, plus la moyenne converge vers \\(3.5\\)\n\n\n\nCode\nlibrary(ggplot2)\n\n# Simuler les lancers de d√©s\nset.seed(200)\nn &lt;- 500\nech &lt;- sample(x = 1:6, size = n, replace = TRUE)\ndata &lt;- data.frame(x = 1:n, \n                   p = sapply(1:n, \n                              function(i) mean(ech[1:i])))\n\n# Tracer le graphique avec ggplot2\nggplot(data, aes(x = x, y = p)) +\n  geom_hline(yintercept = 3.5, \n             color = \"orange\", \n             linetype = \"dashed\",\n             linewidth = 1.1) +\n  geom_line(color = \"darkcyan\", linewidth = 1.2) +\n  labs(title = \"Moyenne des lancers de d√©\",\n       x = \"Nombre de lancers\",\n       y = \"Moyenne\") +\n  xlim(0, n) +\n  ylim(0, 6)"
  },
  {
    "objectID": "doc/tuto/proba/probabilit√©s.html#th√©or√®me-central-limite",
    "href": "doc/tuto/proba/probabilit√©s.html#th√©or√®me-central-limite",
    "title": "Probabilit√©",
    "section": "4 Th√©or√®me Central Limite",
    "text": "4 Th√©or√®me Central Limite\n\n4.1 √ânonc√©\nSoit \\((X_n)_{n\\geq1}\\) une suite de variables al√©atoires iid avec une moyenne \\(\\mu\\) et un √©cart-type \\(\\sigma\\). Alors :\n\\[\\lim_{n \\to +\\infty} \\sqrt{n} \\left( \\frac{1}{n}\\sum_{i=1}^{n} \\frac{ X_i - \\mu}{\\sigma} \\right) \\xrightarrow{\\mathcal {L}} N(0, 1)\\]\nFormulation simple : La diff√©rence entre moyenne empirique et esp√©rance (\\(\\overline{X}_n - \\mathbb{E}(X)\\)) converge en loi vers une loi Normale\n\n\n4.2 Exemple\n\nTirons un √©chantillon de 50 valeurs suivant la loi uniforme sur [0,1] et calculons la moyenne\nR√©p√©tons cette op√©ration 5000 fois et affichons la distribution des moyennes empiriques obtenues\n\n\n\nCode\nlibrary(ggplot2)\n\n# Param√®tres\nset.seed(200)\nnb_sim &lt;- 5000\ntaille_ech &lt;- 50\nu_min &lt;- 0\nu_max &lt;- 1\nE &lt;- (u_max - u_min) / 2\nV &lt;- (u_max - u_min)^2 / 12\n\n# Simulations\nres &lt;- numeric(nb_sim)\nfor (i in 1:nb_sim) {\n  ech &lt;- runif(taille_ech, min = u_min, max = u_max)\n  res[i] &lt;- sqrt(taille_ech) / sqrt(V) * (mean(ech) - E)\n}\n\n# Trac√© de l'histogramme et de la densit√©\nggplot(data.frame(res), aes(x = res)) +\n  geom_histogram(binwidth = 0.2, \n                 fill = \"darkcyan\", \n                 color = \"black\", \n                 aes(y = after_stat(density))) +\n  stat_function(aes(linetype = \"N(0,1)\"), \n                fun = dnorm, \n                args = list(mean = 0, sd = 1), \n                color = \"orange\", \n                linewidth = 1.5) +\n  labs(title = \"Distribution des moyennes empiriques\",\n       x = \"Moyenne des √©chantillons\",\n       y = \"Densit√©\") +\n  scale_linetype_manual(name = \"\", \n                        values = \"dashed\",\n                        labels = c(\"N(0,1)\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n4.3 Vid√©o\nStatQuest by Joshua Starmer"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tutoriels de Statistique",
    "section": "",
    "text": "introduction √† diverses mati√®res\nen utilisant des exemples de code R ou Python"
  },
  {
    "objectID": "index.html#objectifs",
    "href": "index.html#objectifs",
    "title": "Tutoriels de Statistique",
    "section": "",
    "text": "introduction √† diverses mati√®res\nen utilisant des exemples de code R ou Python"
  },
  {
    "objectID": "doc/tuto/proba/lois-probabilit√©-discr√®tes.html",
    "href": "doc/tuto/proba/lois-probabilit√©-discr√®tes.html",
    "title": "Lois de probabilit√© discr√®tes",
    "section": "",
    "text": "1 Loi de Poisson"
  },
  {
    "objectID": "doc/tuto/proba/lois-probabilit√©-continues.html",
    "href": "doc/tuto/proba/lois-probabilit√©-continues.html",
    "title": "Lois de probabilit√© continues",
    "section": "",
    "text": "Moyenne : \\[\\mu\\]\nVariance : \\[\\sigma^2\\]\nDensit√© : \\[f(t)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{1}{2} \\frac{(t-\\mu )^{2}}{\\sigma ^{2}}}\\]\nFonction caract√©ristique : \\[\\phi (t)={\\rm {e}}^{\\mu {\\rm {i}}t-{\\frac {1}{2}}\\sigma ^{2}t^{2}}\\]\nPropri√©t√©s : \\[X_1 + X_2 \\sim \\mathcal{N}(\\mu_1 + \\mu_2,\\sigma_1^2 + \\sigma_2^2) \\Leftrightarrow {\\begin{cases}X_1 \\sim \\mathcal{N}(\\mu_1,\\sigma_1^2) \\\\ X_2 \\sim \\mathcal{N}(\\mu_2,\\sigma_2^2) \\\\ X \\perp \\!\\!\\! \\perp Y \\end{cases}}\\]\n\nDensit√©Fonction de r√©partitionG√©n√©rer un √©chantillon\n\n\n\n\nCode\nlibrary(ggplot2)\n\n# Param√®tres\nmu &lt;- 0\nsigma2 &lt;- 1\n\n# G√©n√©rer des donn√©es\nx &lt;- seq(mu - 3, mu + 3, length.out = 100)\nf_x &lt;- dnorm(x, mean = mu, sd = sigma2)\ndata &lt;- data.frame(x = x, density = f_x)\n\n# Tracer la courbe avec ggplot2\nggplot(data, aes(x = x, y = density)) +\n  geom_line(color = \"darkcyan\", linewidth = 1.5) +\n  scale_x_continuous(name = \"x\", limits = c(-3, 3)) + \n  scale_y_continuous(name = \"f(x)\") + \n  labs(title = \"Densit√© de la loi Normale(0, 1)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\n\n# Param√®tres\na &lt;- 0\nb &lt;- 1\n\n# G√©n√©rer des donn√©es\nx &lt;- seq(mu - 3, mu + 3, length.out = 100)\nF_x &lt;- pnorm(x, mean = mu, sd = sigma2)\ndata &lt;- data.frame(x = x, cdf = F_x)\n\n# Tracer la courbe avec ggplot2\nggplot(data, aes(x = x, y = cdf)) +\n  geom_line(color = \"darkcyan\", linewidth = 1.5) +\n  scale_x_continuous(name = \"x\", limits = c(-3, 3)) + \n  scale_y_continuous(name = \"F(x)\") + \n  labs(title = \"Fonction de r√©partition de la loi Normale(0, 1)\")\n\n\n\n\n\n\n\n\n\n\n\nG√©n√©rer un √©chantillon de n valeurs suivant la loi \\(\\mathcal{N}[0,1]\\)\n\nn &lt;- 5\nech &lt;- rnorm(n, 0, 1)\ndata.frame(ech)"
  },
  {
    "objectID": "doc/tuto/proba/lois-probabilit√©-continues.html#loi-normale",
    "href": "doc/tuto/proba/lois-probabilit√©-continues.html#loi-normale",
    "title": "Lois de probabilit√© continues",
    "section": "",
    "text": "Moyenne : \\[\\mu\\]\nVariance : \\[\\sigma^2\\]\nDensit√© : \\[f(t)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{1}{2} \\frac{(t-\\mu )^{2}}{\\sigma ^{2}}}\\]\nFonction caract√©ristique : \\[\\phi (t)={\\rm {e}}^{\\mu {\\rm {i}}t-{\\frac {1}{2}}\\sigma ^{2}t^{2}}\\]\nPropri√©t√©s : \\[X_1 + X_2 \\sim \\mathcal{N}(\\mu_1 + \\mu_2,\\sigma_1^2 + \\sigma_2^2) \\Leftrightarrow {\\begin{cases}X_1 \\sim \\mathcal{N}(\\mu_1,\\sigma_1^2) \\\\ X_2 \\sim \\mathcal{N}(\\mu_2,\\sigma_2^2) \\\\ X \\perp \\!\\!\\! \\perp Y \\end{cases}}\\]\n\nDensit√©Fonction de r√©partitionG√©n√©rer un √©chantillon\n\n\n\n\nCode\nlibrary(ggplot2)\n\n# Param√®tres\nmu &lt;- 0\nsigma2 &lt;- 1\n\n# G√©n√©rer des donn√©es\nx &lt;- seq(mu - 3, mu + 3, length.out = 100)\nf_x &lt;- dnorm(x, mean = mu, sd = sigma2)\ndata &lt;- data.frame(x = x, density = f_x)\n\n# Tracer la courbe avec ggplot2\nggplot(data, aes(x = x, y = density)) +\n  geom_line(color = \"darkcyan\", linewidth = 1.5) +\n  scale_x_continuous(name = \"x\", limits = c(-3, 3)) + \n  scale_y_continuous(name = \"f(x)\") + \n  labs(title = \"Densit√© de la loi Normale(0, 1)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\n\n# Param√®tres\na &lt;- 0\nb &lt;- 1\n\n# G√©n√©rer des donn√©es\nx &lt;- seq(mu - 3, mu + 3, length.out = 100)\nF_x &lt;- pnorm(x, mean = mu, sd = sigma2)\ndata &lt;- data.frame(x = x, cdf = F_x)\n\n# Tracer la courbe avec ggplot2\nggplot(data, aes(x = x, y = cdf)) +\n  geom_line(color = \"darkcyan\", linewidth = 1.5) +\n  scale_x_continuous(name = \"x\", limits = c(-3, 3)) + \n  scale_y_continuous(name = \"F(x)\") + \n  labs(title = \"Fonction de r√©partition de la loi Normale(0, 1)\")\n\n\n\n\n\n\n\n\n\n\n\nG√©n√©rer un √©chantillon de n valeurs suivant la loi \\(\\mathcal{N}[0,1]\\)\n\nn &lt;- 5\nech &lt;- rnorm(n, 0, 1)\ndata.frame(ech)"
  },
  {
    "objectID": "doc/tuto/proba/lois-probabilit√©-continues.html#loi-uniforme",
    "href": "doc/tuto/proba/lois-probabilit√©-continues.html#loi-uniforme",
    "title": "Lois de probabilit√© continues",
    "section": "2 Loi Uniforme",
    "text": "2 Loi Uniforme\nLoi uniforme continue sur le segment [a,b].\nEsp√©rance : \\[\\mathbb{E}( \\mathcal{U}[a,b]) = \\frac{a+b}{2}\\]\nVariance : \\[\\mathbb{V}( \\mathcal{U}[a,b]) = \\frac{(b-a)^2}{12}\\]\nDensit√© : \\[f(x)=\\frac{1}{b-a} \\mathbb{1}_{a \\leq x \\leq b}(x)\\]\nFonction de r√©partition : \\[F(x)={\\begin{cases}0&{\\text{pour }}x&lt;a\\\\{\\dfrac {x-a}{b-a}}&{\\text{pour }}a\\leq x&lt;b\\\\1&{\\text{pour }}x\\geq b\\end{cases}}\\]\nFonction caract√©ristique : \\[\\phi(t) = \\frac {{\\rm {e}}^{{\\rm {i}}tb}-{\\rm {e}}^{{\\rm {i}}ta}}{{\\rm {i}}t(b-a)}\\]\nCode R\n\nDensit√©Fonction de r√©partitionG√©n√©rer un √©chantillon\n\n\n\n\nCode\nlibrary(ggplot2)\n\n# Param√®tres\na &lt;- 0\nb &lt;- 1\n\n# G√©n√©rer des donn√©es\nx &lt;- seq(a - 0.1, b + 0.1, length.out = 100)\nf_x &lt;- dunif(x, min = a, max = b)\ndata &lt;- data.frame(x = x, density = f_x)\n\n# Tracer la courbe avec ggplot2\nggplot(data, aes(x = x, y = density)) +\n  geom_line(color = \"darkcyan\", linewidth = 1.5) +\n  scale_x_continuous(name = \"x\", breaks = seq(0, 1, by = 0.2), limits = c(-0.1, 1.1)) + \n  scale_y_continuous(name = \"f(x)\") + \n  labs(title = \"Densit√© de la loi Uniforme\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\n\n# Param√®tres\na &lt;- 0\nb &lt;- 1\n\n# G√©n√©rer des donn√©es\nx &lt;- seq(a - 0.1, b + 0.1, length.out = 100)\nF_x &lt;- punif(x, min = a, max = b)\ndata &lt;- data.frame(x = x, cdf = F_x)\n\n# Tracer la courbe avec ggplot2\nggplot(data, aes(x = x, y = cdf)) +\n  geom_line(color = \"darkcyan\", linewidth = 1.5) +\n  scale_x_continuous(name = \"x\", breaks = seq(0, 1, by = 0.2), limits = c(-0.1, 1.1)) + \n  scale_y_continuous(name = \"F(x)\") + \n  labs(title = \"Fonction de r√©partition de la loi Uniforme\")\n\n\n\n\n\n\n\n\n\n\n\nG√©n√©rer un √©chantillon de n valeurs suivant la loi \\(\\mathcal{U}[a,b]\\)\n\nn &lt;- 5\nech &lt;- runif(n, min = 0, max = 1)\ndata.frame(ech)"
  },
  {
    "objectID": "doc/tuto/proba/lois-probabilit√©-continues.html#loi-du-chi2",
    "href": "doc/tuto/proba/lois-probabilit√©-continues.html#loi-du-chi2",
    "title": "Lois de probabilit√© continues",
    "section": "3 Loi du Chi2",
    "text": "3 Loi du Chi2\nLoi du chi 2 √† k degr√©s de libert√©\n\\[\\chi^2(k) \\sim \\sum_{i=1}^{k} X_{i}^{2} \\ avec \\ X_i \\ \\sim \\mathcal{N}(0,1) \\ iid\\]\nEsp√©rance : \\[\\mathbb{E}(\\chi^2(k)) = n\\]\nVariance : \\[\\mathbb{V}(\\chi^2(k)) = 2n\\]\nDensit√© : \\[\\frac{(1/2)^{{k/2}}}{\\Gamma (k/2)} x^{{k/2-1}} e^{{-x/2}}\\]\nFonction caract√©ristique : \\[\\phi(t) = (1-2\\,i\\,t)^{{-k/2}}\\]\nCode R\n\nDensit√©G√©n√©rer un √©chantillon\n\n\n\n\nCode\nlibrary(ggplot2)\n\n# Param√®tres\ndf_values &lt;- c(1, 2, 3, 5, 10, 20)\n\n# Gen√©rer des donn√©es\ndata &lt;- lapply(df_values, function(df) {\n  x &lt;- seq(0, 30, length.out = 200)\n  f_x &lt;- dchisq(x, df = df)\n  data.frame(x = x, density = f_x, df = as.factor(df))\n})\ndata &lt;- do.call(rbind, data)\n\n# Tracer la courbe avec ggplot2\nggplot(data, aes(x = x, y = density, color = df)) +\n  geom_line(linewidth = 1) +\n  scale_x_continuous(name = \"x\", limits = c(0, 30)) + \n  scale_y_continuous(name = \"f(x)\") + \n  labs(title = \"Densit√© de la loi du Chi2 selon diff√©rents degr√©s de libert√©\") +\n  scale_color_discrete(name = \"df\")\n\n\n\n\n\n\n\n\n\n\n\nG√©n√©rer un √©chantillon de n valeurs suivant la loi \\(\\chi^2(df)\\)\n\nn &lt;- 5\ndf &lt;- 3 \nech &lt;- rchisq(n, df)\ndata.frame(ech)"
  },
  {
    "objectID": "doc/tuto/proba/lois-probabilit√©-continues.html#loi-de-student",
    "href": "doc/tuto/proba/lois-probabilit√©-continues.html#loi-de-student",
    "title": "Lois de probabilit√© continues",
    "section": "4 Loi de Student",
    "text": "4 Loi de Student\nLoi de Student √† n degr√©s de libert√©.\n\\[\\mathcal{T}(n) \\sim \\frac{\\mathcal{N}(0,1)}{\\sqrt{\\chi^2(n)/n}}\\]\nEsp√©rance : \\[\\mathbb{E}(\\mathcal{T}(n)) = 0 \\ si \\ n &gt; 1\\]\nVariance : \\[\\mathbb{V}(\\mathcal{T}(n)) = \\frac{n}{n-2} \\ si \\ n &gt; 2 \\ (+\\infty \\ sinon)\\]\nLa loi de Student converge en loi vers la loi Normale \\[\\mathcal{T}(n) \\underset{n \\to +\\infty}{\\overset{\\mathcal{L}} \\longrightarrow} \\mathcal{N}(0,1)\\]\nCode R\n\nDensit√©G√©n√©rer un √©chantillon\n\n\n\n\nCode\nlibrary(ggplot2)\n\n# Param√®tres\ndf_values &lt;- c(1, 2, 5, 20)\n\n# G√©n√©rer des donn√©es\ndata &lt;- lapply(df_values, function(df) {\n  x &lt;- seq(-5, 5, length.out = 200)\n  f_x &lt;- dt(x, df = df)\n  data.frame(x = x, density = f_x, df = as.factor(df))\n})\ndata &lt;- do.call(rbind, data)\n\n# Tracer la courbe avec ggplot2\nggplot(data, aes(x = x, y = density, color = df)) +\n  geom_line(linewidth = 1) +\n  scale_x_continuous(name = \"x\", limits = c(-5, 5)) + \n  scale_y_continuous(name = \"f(x)\") + \n  labs(title = \"Densit√© de la loi de Student selon diff√©rents degr√©s de libert√©\") +\n  scale_color_discrete(name = \"df\")\n\n\n\n\n\n\n\n\n\n\n\nG√©n√©rer un √©chantillon de n valeurs suivant la loi \\(\\mathcal{T}(df)\\)\n\nn &lt;- 5\ndf &lt;- 3 \nech &lt;- rt(n, df)\ndata.frame(ech)"
  },
  {
    "objectID": "doc/tuto/proba/lois-probabilit√©-continues.html#loi-de-fisher-snedecor",
    "href": "doc/tuto/proba/lois-probabilit√©-continues.html#loi-de-fisher-snedecor",
    "title": "Lois de probabilit√© continues",
    "section": "5 Loi de Fisher-Snedecor",
    "text": "5 Loi de Fisher-Snedecor\nLoi de Fisher-Snedecor √† n et m degr√©s de libert√©\n\\(\\mathcal{F}(n,m) \\sim \\frac{\\chi^2(n)/n}{\\chi^2(m)/m}\\)\n\\(\\mathbb{E}(\\mathcal{F}(n,m)) = \\frac {m}{m-2}\\) si \\(m &gt; 2\\)\n\\(X \\sim \\mathcal{F}(a,b) \\Rightarrow \\frac{1}{X} \\sim \\mathcal{F}(b,a)\\)\nLien avec la loi de Student : \\(X\\sim \\mathcal{T}(n) \\Rightarrow X^2 \\sim \\mathcal{F}(1,n)\\)\nLien avec la loi Normale : \\(X \\sim \\mathcal{N}(0,1) \\Rightarrow X^2 \\sim \\mathcal{F}(1,\\infty)\\)"
  },
  {
    "objectID": "doc/tuto/proba/lois-probabilit√©-continues.html#loi-exponentielle",
    "href": "doc/tuto/proba/lois-probabilit√©-continues.html#loi-exponentielle",
    "title": "Lois de probabilit√© continues",
    "section": "6 Loi Exponentielle",
    "text": "6 Loi Exponentielle\nLoi exponentielle de param√®tre Lambda\n\\(\\mathbb{E}(\\epsilon(\\lambda)) = \\frac{1}{\\lambda}\\)\n\\(\\mathbb{V}(\\epsilon(\\lambda)) = \\frac{1}{\\lambda^2}\\)\nDensit√© : \\(\\lambda e^{{-\\lambda x}} \\mathbb{1}_{x \\geq 0}\\)\nFonction de r√©partition : \\(1-e^{{-\\lambda x}}\\)\nFonction caract√©ristique : \\(\\left(1-{\\dfrac {it}{\\lambda }}\\right)^{{-1}}\\)"
  },
  {
    "objectID": "doc/tuto/proba/lois-probabilit√©-continues.html#loi-gamma",
    "href": "doc/tuto/proba/lois-probabilit√©-continues.html#loi-gamma",
    "title": "Lois de probabilit√© continues",
    "section": "7 Loi Gamma",
    "text": "7 Loi Gamma\nLoi Gamma de param√®tres alpha et beta\n\\(X \\sim \\Gamma( \\alpha , \\beta)\\)\n\\(\\mathbb{E}(\\Gamma( \\alpha , \\beta)) = \\frac{\\alpha}{\\beta}\\)\n\\(\\mathbb{V}(\\Gamma( \\alpha , \\beta)) = \\frac{\\alpha}{\\beta^2}\\)\nDensit√© : \\(f ( x , \\alpha, \\beta ) = x^{\\alpha -1} \\frac{\\beta ^\\alpha e^{-\\beta x}}{\\Gamma (\\alpha )} \\mathbb{1}_{x &gt; 0}\\)\nLiens avec d‚Äôautres lois : - Si \\(\\alpha = 1 \\sim\\) loi exponentielle - Si \\(\\Gamma (n / 2 , 1/2 ) \\sim\\) loi du \\(\\chi^2\\) √† \\(n\\) degr√©s de libert√©\n\n7.1 Fonction Gamma\n\\(\\Gamma(a) = \\int _{0}^{+\\infty }t^{a-1}\\,\\mathrm {e} ^{-t}\\,\\mathrm dt\\)\nProlonge la fonction factorielle √† l‚Äôensemble des nombres complexes (sauf entiers n√©gatifs)\n\\(\\Gamma(n+1) = n!\\)\n\\(\\Gamma(x+1) = x\\Gamma(x)\\)\n\n\n7.2 Loi inverse Gamma\nLoi inverse Gamma de param√®tres k et \\(\\theta\\)\n\\(X \\sim {{Inv \\ \\Gamma}(k,\\theta )} \\Rightarrow \\frac{1}{X} \\sim \\Gamma ( k , 1 / \\theta )\\)\nDensit√© : \\(f(x;\\alpha ,\\beta )={\\frac {\\beta ^{\\alpha }}{\\Gamma (\\alpha )}}(1/x)^{\\alpha +1}\\exp \\left(-\\beta /x\\right)\\) pour \\(x &gt; 0\\)\n\\(\\mathbb{E}(Inv \\ \\Gamma( \\alpha , \\beta)) = \\frac{\\beta}{\\alpha-1}\\) pour \\(\\alpha &gt; 1\\)\n\\(\\mathbb{V}(Inv \\ \\Gamma( \\alpha , \\beta)) = \\frac {\\beta ^{2}}{(\\alpha -1)^{2}(\\alpha -2)}\\) pour \\(\\alpha &gt; 2\\)"
  },
  {
    "objectID": "doc/tuto/proba/lois-probabilit√©-continues.html#loi-beta",
    "href": "doc/tuto/proba/lois-probabilit√©-continues.html#loi-beta",
    "title": "Lois de probabilit√© continues",
    "section": "8 Loi Beta",
    "text": "8 Loi Beta\nLoi Beta de param√®tres alpha et beta\n\\(\\mathbb{E}(\\mathrm{B}) = \\frac {\\alpha }{\\alpha +\\beta }\\)\n\\(\\mathbb{V}(\\mathrm{B}) = \\frac {\\alpha \\beta }{(\\alpha +\\beta )^{2}(\\alpha +\\beta +1)}\\)\nDensit√© : \\(\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{\\mathrm{B} (\\alpha ,\\beta )}\\)\n\n8.1 Fonction Beta\n\\(\\mathrm {B} (x,y)=\\int _{0}^{1}t^{x-1}(1-t)^{y-1}\\mathrm {d} t\\)\n\\(\\mathrm {B} (x,y)={\\frac {\\Gamma (x)\\,\\Gamma (y)}{\\Gamma (x+y)}}\\)\n\\(\\mathrm {B} (x,y)=\\mathrm {B} (y,x)\\)\n\\(\\mathrm {B} (x,y+1)=\\frac{y}{x+y}\\mathrm {B} (x,y)\\)"
  }
]