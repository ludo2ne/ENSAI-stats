[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tutoriels de Statistique",
    "section": "",
    "text": "introduction √† diverses mati√®res\nen utilisant des exemples de code R ou Python"
  },
  {
    "objectID": "index.html#objectifs",
    "href": "index.html#objectifs",
    "title": "Tutoriels de Statistique",
    "section": "",
    "text": "introduction √† diverses mati√®res\nen utilisant des exemples de code R ou Python"
  },
  {
    "objectID": "index.html#todo",
    "href": "index.html#todo",
    "title": "Tutoriels de Statistique",
    "section": "üöß Todo",
    "text": "üöß Todo\nReprendre le contenu de https://github.com/ludo2ne/R-tuto et mettre au format qmd\n\nLois de probabilit√© discr√©tes\nClassification non supervis√©e\nStatistique inf√©rentielle\nApprentissage Supervis√©\nMod√®les lin√©aires g√©n√©ralis√©s\nS√©ries Temporelles\nMod√®les de Survie\nSondages\nChaines de Markov\nInf√©rence Bay√©sienne"
  },
  {
    "objectID": "index.html#work-in-progress",
    "href": "index.html#work-in-progress",
    "title": "Tutoriels de Statistique",
    "section": "Work in progress",
    "text": "Work in progress\n\nM√©thodes factorielles\nTests Statistiques\nProbabilit√©s\nR√©gression lin√©aire"
  },
  {
    "objectID": "index.html#done",
    "href": "index.html#done",
    "title": "Tutoriels de Statistique",
    "section": "Done",
    "text": "Done\n\nLois de probabilit√© continues"
  },
  {
    "objectID": "doc/tuto/stats/tests-statitiques.html",
    "href": "doc/tuto/stats/tests-statitiques.html",
    "title": "Tests Statistiques",
    "section": "",
    "text": "Le but du test est de rejeter \\(H_0\\) avec la plus petite erreur possible."
  },
  {
    "objectID": "doc/tuto/stats/tests-statitiques.html#types-de-tests",
    "href": "doc/tuto/stats/tests-statitiques.html#types-de-tests",
    "title": "Tests Statistiques",
    "section": "1 Types de tests",
    "text": "1 Types de tests\n\nselon hypoth√®ses\n\nsimple vs.¬†simple : \\(H_0 : \\theta = \\theta_0 \\  vs \\  H_1 : \\theta = \\theta_1\\)\nsimple vs.¬†multiple (bilat√©ral): \\(H_0 : \\theta = \\theta_0 \\  vs \\  H_1 : \\theta \\ne \\theta_0\\)\nmultiple vs.¬†simple (unilat√©ral): \\(H_0 : \\theta \\le \\theta_0 \\  vs \\  H_1 : \\theta &gt; \\theta_0\\)\n\nselon loi param√©trique ou non\nobjet test√© : moyenne, variance, ind√©pendance‚Ä¶"
  },
  {
    "objectID": "doc/tuto/stats/tests-statitiques.html#zone-de-rejet",
    "href": "doc/tuto/stats/tests-statitiques.html#zone-de-rejet",
    "title": "Tests Statistiques",
    "section": "2 Zone de rejet",
    "text": "2 Zone de rejet\n\n\\(T\\) : Stat de test \\(=f¬∞(echantillon)\\)\n\\(\\mathcal{R}\\) : Zone de rejet\n\nde faible probabilit√© pour T sachant \\(H_0\\)\nSi \\(t \\in \\mathcal{R}\\) : on rejette \\(H_0\\) (o√π t est la r√©alisation de T)\n\nRisque de 1ere esp√®ce : \\(\\alpha = \\mathbb{P}(T \\in \\mathcal{R} \\ | \\ H_0)\\)\n\n\\(\\alpha\\) (Niveau de test) = Probabilit√© de rejeter \\(H_0\\) alors qu‚Äôelle est vraie\nc‚Äôest l‚Äôerreur la plus grave qu‚Äôil faut surtout √©viter\nexemple : d√©clarer coupable un innocent\n\nRisque de 2e esp√®ce : \\(\\beta\\) tel que \\(\\beta = \\mathbb{P}(T \\in \\overline{\\mathcal{R}} \\ | \\ H_1)\\)\n\nProbabilit√© d‚Äôaccepter \\(H_0\\) alors qu‚Äôelle est fausse\nexemple : d√©clarer innocent un coupable\n\nPuissance d'un test : probabilit√© de rejeter \\(H_0\\) alors qu‚Äôelle est fausse\n\n\\(1 - \\beta = \\mathbb{P}(T \\in \\mathcal{R} \\ | \\ H_1)\\)\n\nUn test est sans biais si \\(1 - \\beta \\ge \\alpha\\) (Puissance \\(\\ge\\) Niveau de test)\n\\(T_1\\) est plus puissant que \\(T_2\\) si pour un m√™me niveau de test \\(\\alpha\\), \\(1 - \\beta_1 \\ge 1 - \\beta_2\\)"
  },
  {
    "objectID": "doc/tuto/stats/tests-statitiques.html#etude-dun-cas-concret",
    "href": "doc/tuto/stats/tests-statitiques.html#etude-dun-cas-concret",
    "title": "Tests Statistiques",
    "section": "3 Etude d‚Äôun cas concret",
    "text": "3 Etude d‚Äôun cas concret\nüéØ Objectif : nous souhaitons tester si une pi√®ce est non truqu√©e.\nNous utiliserons une loi de Bernoulli pour mod√©liser nos lancers, avec par exemple :\n\npile : 1\nface : 0\n\n\nset.seed(1986)\n\nn &lt;- 100     # nombre de lancers\np &lt;- 0.5     # Probabilit√© d'avoir un pile\n\n# Simulation de n lancers\nX &lt;- rbinom(n, 1, prob = p)\n\n# Quelques statistiques descriptives\ntable(X)\n\nX\n 0  1 \n49 51 \n\nprop.table(table(X))\n\nX\n   0    1 \n0.49 0.51 \n\nmean(X)\n\n[1] 0.51\n\nvar(X)     # p * (1-p)\n\n[1] 0.2524242\n\n\n\n3.1 LFGN\nLa Loi Forte des Grands Nombres nous dit que :\n\nplus nous augmentons le nombre de lancers\nplus la moyenne empirique converge vers l‚Äôesp√®rance\n\nCalculons les moyennes cumul√©es et tra√ßons l‚Äô√©volution lorsque le nombre de lancers augmente\n\n# Somme cumulative divis√©e par le nombre de lancers correspondants\nx_bar &lt;- cumsum(X) / seq_along(X)\n\ndf &lt;- data.frame(x = seq_along(x_bar), x_bar = x_bar)\n\n\n\nAutre m√©thode pour calculer x_bar\nx_bar &lt;- rep(NA, n)\nfor (i in 1:n) {\n  x_bar[i] &lt;- sum(X[1:i]) / i\n}\n\n\n\n\nCode ggplot\nlibrary(ggplot2)\n\nggplot(df, aes(x, x_bar)) +\n  geom_line(color = \"darkcyan\", \n            linewidth = 1.5) +\n  geom_hline(yintercept = p, \n             linetype = \"dashed\", \n             color = \"darkorange\") +\n  labs(x = \"Nombre de lancers\", \n       y = \"Moyenne\", \n       title = \"√âvolution de la moyenne selon le nombre de lancers de pi√®ce\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n3.2 TCL\nApplication du Th√©or√®me Central Limite :\n\nnous r√©p√®tons L fois l‚Äôexp√©rience de n lancers\n√† chaque fois, calculons la moyenne obtenue\npuis nous normalisons notre √©chantillon\nenfin, tra√ßons l‚Äôhistogramme des moyennes\n\n\nset.seed(1986)\n\nL &lt;- 1000\nn &lt;- 100\np &lt;- 0.5\n\nx_bar &lt;- numeric(L)\nfor (i in 1:L) {\n  X &lt;- rbinom(n, 1, prob = p)\n  x_bar[i] &lt;- mean(X)\n}\n\n# Normalisation\nY &lt;- sqrt(n) * (x_bar - p) / sqrt(p * (1 - p))\n\n\n\nCode ggplot\nlibrary(ggplot2)\n\nggplot(data.frame(Y), aes(x = Y)) +\n  geom_histogram(binwidth = 0.20, \n                 color = \"black\", \n                 fill = \"darkcyan\") +\n  labs(x = \"Valeurs normalis√©es\", \n       y = \"Fr√©quence\", \n       title = \"Histogramme des valeurs normalis√©es\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nLa distribution nous fait bien penser √† la loi normale centr√©e r√©duite.\n\n\nCode ggplot\ndensity_Y &lt;- density(Y)\n\n# Create data for standard normal density curve\nstd_norm_curve &lt;- data.frame(x = seq(-4, 4, length.out = 100),\n                             y = dnorm(seq(-4, 4, length.out = 100)))\n\n\nlegend_colors &lt;- c(\"Densit√© empirique\" = \"darkcyan\", \"Densit√© N(0,1)\" = \"darkorange\")\n\n# Create ggplot with density plot and standard normal density curve\nggplot() +\n  geom_line(data = data.frame(x = density_Y$x, \n                              y = density_Y$y), \n            aes(x, y, color = \"Densit√© empirique\"),\n            linewidth = 1.5) +\n  geom_line(data = std_norm_curve, \n            aes(x, y, color = \"Densit√© N(0,1)\"), \n            linetype = \"dashed\", \n            linewidth = 1) +\n  xlim(-4, 4) +\n  labs(x = \"Valeurs normalis√©es\", \n       y = \"Densit√©\", \n       title = \"Comparaison des densit√©s empiriques et th√©oriques\",\n       color = \"L√©gende\") + \n  scale_color_manual(values = legend_colors) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n3.3 Mon premier test\nSupposons que nous lan√ßons 100 fois une pi√®ce et que nous obtenons :\n\n59 pile (1)\n41 face (0)\n\n\nn &lt;- 100\nnb_pile &lt;- 59\nX &lt;- c(rep(1, nb_pile), rep(0, n - nb_pile))\ntable(X)\n\nX\n 0  1 \n41 59 \n\n\nOk, il y a un √©cart significatif entre le nombre de piles et de faces, mais est-ce que cela signifie que la pi√®ce est truqu√©e ?\nNous allons √©tablir un test pour voir si la moyenne empirique obtenue s‚Äô√©loigne trop de la moyenne th√©orique.\n\nSoit \\(H_0\\) : la pi√®ce n‚Äôest pas truqu√©e (p = 0.5)\nVs \\(H_1\\) : la pi√®ce est truqu√©e (p ‚â† 0.5)\n\nNous d√©finissons le niveau de risque \\(\\alpha\\).\nUn niveau de risque √† 0.05 signifie que l‚Äôon veut √™tre s√ªr √† 95 % de ne pas rejeter \\(H_0\\) alors qu‚Äôelle est vraie.\n\nalpha &lt;- 0.05\np_hat &lt;- mean(X)\n\nSoit \\(T\\) la statistique de test\n\ncalcul√©e √† partir des donn√©es observ√©es,\nutilis√©e pour √©valuer la plausibilit√© de l‚Äôhypoth√®se nulle\n\nsert √† quantifier l‚Äô√©cart entre les donn√©es observ√©es et ce √† quoi on s‚Äôattendrait si l‚Äôhypoth√®se nulle √©tait vraie\n\n\n\\[T = \\frac{\\hat{p} - p}{\\sqrt{\\frac{p(1-p)}{n}}}\\]\n\nT &lt;- sqrt(n) * (p_hat - 0.5) / sqrt(p * (1 - p)); T\n\n[1] 1.8\n\n\nSous \\(H_0\\), \\(T\\) suit une loi Normale Centr√©e R√©duite :\n\nnous d√©finissons donc la zone de rejet\n\nla zone de rejet est la r√©gion o√π les valeurs observ√©es sont tellement extr√™mes que notre hypoth√®se de d√©part semble fausse\n\nnous rejetons \\(H_0\\) si T n‚Äôappartient pas √† l‚Äôintervalle suivant\n\n\nc(-1, 1) * qnorm(1 - alpha / 2)\n\n[1] -1.959964  1.959964\n\n\nEst-ce que l‚Äôon rejette \\(H_0\\) ?\nAutrement dit, est-ce que \\(T\\) appartient √† la zone de rejet ?\n\nT &lt; -qnorm(1 - alpha / 2) | T &gt; qnorm(1 - alpha / 2)\n\n[1] FALSE\n\n\nOn ne peut pas rejeter \\(H_0\\) au niveau de risque 0.05.\nRepr√©sentation graphique :\n\n\nCode ggplot\n# Donn√©es pour la courbe de densit√© normale\nx_values &lt;- seq(-4, 4, length.out = 100)\ny_values &lt;- dnorm(x_values)\n\n# Donn√©es pour les zones de rejet\nzone_rejet_gauche &lt;- seq(-4, qnorm(alpha/2), 0.01)\nzone_rejet_droite &lt;- seq(qnorm(1-alpha/2), 4, 0.01)\n\n# Cr√©ation du graphique avec ggplot2\nggplot() +\n  geom_line(data = data.frame(x = x_values, \n                              y = y_values), \n            aes(x, y), \n            color = \"darkcyan\", \n            linewidth = 1.5) +\n  geom_polygon(data = data.frame(x = c(zone_rejet_gauche, \n                                       zone_rejet_gauche[length(zone_rejet_gauche)]), \n                                 y = c(dnorm(zone_rejet_gauche), 0)), \n               aes(x, y), \n               fill = \"darkorange\") +\n  geom_polygon(data = data.frame(x = c(zone_rejet_droite, \n                                       zone_rejet_droite[length(zone_rejet_droite)]), \n                                 y = c(0, dnorm(zone_rejet_droite))), \n               aes(x, y), \n               fill = \"darkorange\") +\n  geom_rug(data = data.frame(T = T), \n           aes(x = T), \n           sides = \"b\", \n           color = \"blue\", \n           linewidth = 1) +\n  xlim(-4, 4) +\n  labs(x = \"x\", y = \"Densit√©\", title = \"Loi de T sous H0\") +\n  theme_minimal() +\n  scale_fill_manual(values = \"darkorange\", guide = FALSE) +\n  guides(color = \"none\") +\n  annotate(\"text\", x = 3, y = 0.1, label = \"Zones de rejet\", color = \"darkorange\") +\n  annotate(\"text\", x = T, y = 0.02, label = \"T\", color = \"blue\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nNous remarquons que \\(T\\) ne se trouve pas dans la zone de rejet\nSi \\(T\\) se trouvait dans la zone de rejet, nous aurions affirm√© qu‚Äôil est peu probable que \\(T\\) suive une \\(\\mathcal{N}(0, 1)\\)\n\nnous aurions donc rejet√© \\(H_0\\)\n\n\n\n\n3.4 p-valeur\n\nProbabilit√© d‚Äôobtenir une valeur aussi extr√™me que celle observ√©e\nplus grande valeur possible de \\(\\alpha\\) conduisant √† ne pas rejeter \\(H_0\\)\n\n\np_val &lt;- 2 * (1 - pnorm(T)) ; p_val\n\n[1] 0.07186064\n\n\nLa p-valeur vaut environs 11%. Cela signifie que si nous avions choisi un niveau de risque de :\n\n10% : nous ne rejettons pas \\(H_0\\) et consid√©rons que la pi√®ce n‚Äôest pas truqu√©e\n12% : nous rejettons \\(H_0\\)\n\n\n\n3.5 Test unilat√©ral\nJusqu‚Äô√† maintenant, nous √©tions dans le cas d‚Äôun test bilat√©ral.\nC‚Äôest √† dire que nous testions pour d√©terminer si la pi√®ce est truqu√©e, sans nous pr√©occuper si elle avantage le c√¥t√© pile ou le c√¥t√© face.\nMaintenant, imaginons que nous souhaitons tester si elle est truqu√©e c√¥t√© pile uniquement. Nous avons donc :\n\nSoit \\(H_0\\) : la pi√®ce n‚Äôest pas truqu√©e (p = 0.5)\nVs \\(H_1\\) : la pi√®ce est truqu√©e c√¥t√© pile (p &gt; 0.5)\n\nLa statistique de test \\(T\\) ne change pas.\nNous rejetons si p est trop grand, donc la zone de rejet est :\n\nc(qnorm(1 - alpha), Inf)\n\n[1] 1.644854      Inf\n\n\nEst-ce que l‚Äôon rejette \\(H_0\\) ?\n\nT &gt; qnorm(1 - alpha)\n\n[1] TRUE\n\n\nAu niveau de risque 0.05, nous rejetons \\(H_0\\) et donc la pi√®ce est truqu√©e c√¥t√© pile.\nRepr√©sentation graphique :\n\n\nCode ggplot\n# Donn√©es pour la courbe de densit√© normale\nx_values &lt;- seq(-4, 4, length.out = 100)\ny_values &lt;- dnorm(x_values)\n\n# Donn√©es pour les zones de rejet\nzone_rejet_droite &lt;- seq(qnorm(1-alpha), 4, 0.01)\n\n# Cr√©ation du graphique avec ggplot2\nggplot() +\n  geom_line(data = data.frame(x = x_values, \n                              y = y_values), \n            aes(x, y), \n            color = \"darkcyan\", \n            linewidth = 1.5) +\n  geom_polygon(data = data.frame(x = c(zone_rejet_droite, \n                                       zone_rejet_droite[length(zone_rejet_droite)]), \n                                 y = c(0, dnorm(zone_rejet_droite))), \n               aes(x, y), \n               fill = \"darkorange\") +\n  geom_rug(data = data.frame(T = T), \n           aes(x = T), \n           sides = \"b\", \n           color = \"blue\", \n           linewidth = 1) +\n  xlim(-4, 4) +\n  labs(x = \"x\", y = \"Densit√©\", title = \"Loi de T sous H0\") +\n  theme_minimal() +\n  scale_fill_manual(values = \"darkorange\", guide = FALSE) +\n  guides(color = \"none\") +\n  annotate(\"text\", x = 3, y = 0.1, label = \"Zones de rejet\", color = \"darkorange\") +\n  annotate(\"text\", x = T, y = 0.02, label = \"T\", color = \"blue\") +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "doc/tuto/stats/tests-statitiques.html#vid√©os",
    "href": "doc/tuto/stats/tests-statitiques.html#vid√©os",
    "title": "Tests Statistiques",
    "section": "4 Vid√©os",
    "text": "4 Vid√©os\nStatQuest by Joshua Starmer"
  },
  {
    "objectID": "doc/tuto/stats/analyse-multivariee.html",
    "href": "doc/tuto/stats/analyse-multivariee.html",
    "title": "Analyse multivari√©e",
    "section": "",
    "text": "Les exemples utilis√©s sont repris du cours de Matthieu MARBAC dispens√© √† l‚ÄôENSAI en 2021.\n\nL‚Äôanalyse de donn√©es sur UtilitR"
  },
  {
    "objectID": "doc/tuto/stats/analyse-multivariee.html#documentation-et-sources",
    "href": "doc/tuto/stats/analyse-multivariee.html#documentation-et-sources",
    "title": "Analyse multivari√©e",
    "section": "",
    "text": "Les exemples utilis√©s sont repris du cours de Matthieu MARBAC dispens√© √† l‚ÄôENSAI en 2021.\n\nL‚Äôanalyse de donn√©es sur UtilitR"
  },
  {
    "objectID": "doc/tuto/stats/analyse-multivariee.html#objectifs-et-contexte",
    "href": "doc/tuto/stats/analyse-multivariee.html#objectifs-et-contexte",
    "title": "Analyse multivari√©e",
    "section": "Objectifs et Contexte",
    "text": "Objectifs et Contexte\n\nVous disposez des donn√©es suivantes\n\n\\(n\\) individus\n\\(p\\) variables\n\\(X\\) : matrice des donn√©es de dimension \\(n\\) x \\(p\\)\n\nComment analyser la structure et repr√©senter ces donn√©es ?\n\nIl n‚Äôai pas ais√© de repr√©senter vos donn√©es en dimension \\(p\\) :\n\nNuage de points\n\nrepr√©sentation de vos donn√©es en dimension p\npour les calculs, le nuage de point sera centr√© et r√©duit\n\ncela permet d‚Äôavoir le m√™me poids pour chaque variable\npar exemple en r√©duisant, une variable age (valeurs entre 0 et 120) aura la m√™me importance qu‚Äôune variable nombre de neurones (valeurs entre 100 millions et 100 milliards)\nce n‚Äôest pas obligatoire de r√©duire, mais si on ne le fait pas, la variable age dont la variance est beaucoup plus faible, sera n√©gligeable\n\n\n\nüéØ Vous souhaitez par exemple repr√©senter vos donn√©es sur un graphique en 2D.\nLes m√©thodes factorielles sont l√† pour vous aider √† r√©duire la complexit√© des donn√©es tout en pr√©servant au maximum l‚Äôinformation qu‚Äôelles contiennent.\nComment ? En effectuant un changement de base pour obtenir des axes o√π l‚Äôinertie est maximale.\n\nInertie\n\nrepr√©sente l‚Äô√©cart entre les observations\ngrande si le nuage est dispers√©, petite sinon\nl‚Äôinertie totale est √©gale √† la somme des inerties de chaque axe\n\n\n\n\n\n\n\n\nAutrement dit\n\n\n\n\nVous partez d‚Äôun nuage de point de dimension \\(\\mathbb{R}^p\\)\nVous effectuez un changement de base\nPour le choix du 1er axe factoriel, vous cherchez l‚Äôaxe qui maximise l‚Äôinertie\nPuis vous cherchez un 2e axe factoriel qui maximise l‚Äôinertie restante\netc. jusqu‚Äôau dernier axe (p)\nEnfin, vous analysez les premiers axes, ceux qui concentrent la majorit√© de l‚Äôinertie\n\n\n\nDans le domaine de l‚Äôanalyse des donn√©es multivari√©es, plusieurs techniques sont couramment utilis√©es pour explorer la structure sous-jacente des donn√©es et en extraire des informations significatives.\nQuatre de ces techniques employ√©es sont :\n\nl‚ÄôAnalyse en Composantes Principales (ACP)\nl‚ÄôAnalyse Factorielle Discriminante (AFD)\nl‚ÄôAnalyse Factorielle des Correspondances (AFC)\nl‚ÄôAnalyse des Correspondances Multiples (ACM)"
  },
  {
    "objectID": "doc/tuto/stats/analyse-multivariee.html#ma-premi√®re-acp",
    "href": "doc/tuto/stats/analyse-multivariee.html#ma-premi√®re-acp",
    "title": "Analyse multivari√©e",
    "section": "1 Ma premi√®re ACP",
    "text": "1 Ma premi√®re ACP\nNous allons commencer par r√©aliser une ACP. Les autres m√©thodes suivent globalement le m√™me principe avec quelques variantes.\nPour r√©aliser une ACP, nous ne gardons que des variables quantitatives.\n\n\n\n\n\n\nImportant\n\n\n\nNous nous baserons sur un cas simple o√π :\n\ntous les individus ont le m√™me poids, c‚Äôest √† dire la m√™me importance\ntoutes les variables ont le m√™me poids (M√©trique : Identit√©)\n\n\n\n\n1.1 Donn√©es\nCommen√ßons par r√©initialiser notre environnement et charger les librairies utiles\n\nrm(list=ls())\n\nlibrary(ggplot2)\nlibrary(FactoMineR)         # install.packages(\"FactoMineR\")\nlibrary(factoextra)         # install.packages(\"factoextra\")\n\nNous utilisons un jeu de donn√©es contenant les temp√©ratures de diff√©rentes villes d‚ÄôEurope\n\ntemperature &lt;- read.table(\"data/temperature.csv\", \n                          header = TRUE,\n                          sep = \";\",\n                          row.names = 1,\n                          stringsAsFactors = TRUE)\n\nstr(temperature)\n\n'data.frame':   35 obs. of  17 variables:\n $ Janvier  : num  2.9 9.1 -0.2 3.3 -1.1 -0.4 4.8 -5.8 -5.9 -3.7 ...\n $ Fevrier  : num  2.5 9.7 0.1 3.3 0.8 -0.4 5 -6.2 -5 -2 ...\n $ Mars     : num  5.7 11.7 4.4 6.7 5.5 1.3 5.9 -2.7 -0.3 1.9 ...\n $ Avril    : num  8.2 15.4 8.2 8.9 11.6 5.8 7.8 3.1 7.4 7.9 ...\n $ Mai      : num  12.5 20.1 13.8 12.8 17 11.1 10.4 10.2 14.3 13.2 ...\n $ Juin     : num  14.8 24.5 16 15.6 20.2 15.4 13.3 14 17.8 16.9 ...\n $ Juillet  : num  17.1 27.4 18.3 17.8 22 17.1 15 17.2 19.4 18.4 ...\n $ Aout     : num  17.1 27.2 18 17.8 21.3 16.6 14.6 14.9 18.5 17.6 ...\n $ Septembre: num  14.5 23.8 14.4 15 16.9 13.3 12.7 9.7 13.7 13.7 ...\n $ Octobre  : num  11.4 19.2 10 11.1 11.3 8.8 9.7 5.2 7.5 8.6 ...\n $ Novembre : num  7 14.6 4.2 6.7 5.1 4.1 6.7 0.1 1.2 2.6 ...\n $ Decembre : num  4.4 11 1.2 4.4 0.7 1.3 5.4 -2.3 -3.6 -1.7 ...\n $ Moyenne  : num  9.9 17.8 9.1 10.3 10.9 7.8 9.3 4.8 7.1 7.7 ...\n $ Amplitude: num  14.6 18.3 18.5 14.4 23.1 17.5 10.2 23.4 25.3 22.1 ...\n $ Latitude : num  52.2 37.6 52.3 50.5 47.3 55.4 53.2 60.1 50.3 50 ...\n $ Longitude: num  4.5 23.5 13.2 4.2 19 12.3 6.1 25 30.3 19.6 ...\n $ Region   : Factor w/ 4 levels \"Est\",\"Nord\",\"Ouest\",..: 3 4 3 3 1 2 2 2 1 1 ...\n\n\nPour chacune des villes, nous avons les variables suivantes :\n\n12 variables pour les temp√©ratures moyennes pour chaque mois\nla temp√©rature moyenne de l‚Äôann√©e, ainsi que l‚Äôamplitude\nla latitude et la longitude\nla r√©gion\n\nToutes les variables sont num√©riques, except√© la r√©gion.\n\n\n1.2 Statistiques descriptives\nAvant de se lancer t√™te baiss√©e, regardons les statistiques descriptives\n\nsummary(temperature)\n\n    Janvier          Fevrier            Mars            Avril       \n Min.   :-9.300   Min.   :-7.900   Min.   :-3.700   Min.   : 2.900  \n 1st Qu.:-1.550   1st Qu.:-0.150   1st Qu.: 1.600   1st Qu.: 7.250  \n Median : 0.200   Median : 1.900   Median : 5.400   Median : 8.900  \n Mean   : 1.346   Mean   : 2.217   Mean   : 5.229   Mean   : 9.283  \n 3rd Qu.: 4.900   3rd Qu.: 5.800   3rd Qu.: 8.500   3rd Qu.:12.050  \n Max.   :10.700   Max.   :11.800   Max.   :14.100   Max.   :16.900  \n      Mai             Juin          Juillet           Aout      \n Min.   : 6.50   Min.   : 9.30   Min.   :11.10   Min.   :10.60  \n 1st Qu.:12.15   1st Qu.:15.40   1st Qu.:17.30   1st Qu.:16.65  \n Median :13.80   Median :16.90   Median :18.90   Median :18.30  \n Mean   :13.91   Mean   :17.41   Mean   :19.62   Mean   :18.98  \n 3rd Qu.:16.35   3rd Qu.:19.80   3rd Qu.:21.75   3rd Qu.:21.60  \n Max.   :20.90   Max.   :24.50   Max.   :27.40   Max.   :27.20  \n   Septembre        Octobre         Novembre         Decembre    \n Min.   : 7.90   Min.   : 4.50   Min.   :-1.100   Min.   :-6.00  \n 1st Qu.:13.00   1st Qu.: 8.65   1st Qu.: 3.200   1st Qu.: 0.25  \n Median :14.80   Median :10.20   Median : 5.100   Median : 1.70  \n Mean   :15.63   Mean   :11.00   Mean   : 6.066   Mean   : 2.88  \n 3rd Qu.:18.25   3rd Qu.:13.30   3rd Qu.: 7.900   3rd Qu.: 5.40  \n Max.   :24.30   Max.   :19.40   Max.   :14.900   Max.   :12.00  \n    Moyenne        Amplitude        Latitude       Longitude       Region  \n Min.   : 4.50   Min.   :10.20   Min.   :37.20   Min.   : 0.00   Est  : 8  \n 1st Qu.: 7.75   1st Qu.:14.90   1st Qu.:43.90   1st Qu.: 4.35   Nord : 8  \n Median : 9.70   Median :18.50   Median :50.00   Median : 9.40   Ouest: 9  \n Mean   :10.27   Mean   :18.32   Mean   :48.77   Mean   :11.98   Sud  :10  \n 3rd Qu.:12.65   3rd Qu.:21.45   3rd Qu.:52.75   3rd Qu.:18.65             \n Max.   :18.20   Max.   :27.60   Max.   :64.10   Max.   :30.30             \n\n\n\nggplot(temperature, aes(x = Region, y = Moyenne)) +\n  geom_point(size = 3, color=\"darkcyan\") +\n  labs(title = \"Temp√©ratures moyennes annuelles selon la r√©gion\",\n       x = \"R√©gion\",\n       y = \"Temp√©rature moyenne (¬∞C)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n1.3 S√©lection des variables\nLa premi√®re √©tape consiste √† s√©lectionner les variables et individus qui vont servir √† r√©aliser l‚ÄôACP.\n\nvariables actives : variables qui serviront √† calculer les nouveaux axes\nindividus actifs : individus qui serviront √† calculer les nouveaux axes\nindividus et variables suppl√©mentaires : individus et variables qui ne serviront pas √† la cr√©ation des nouveaux axes mais dont on souhaite observer la position sur ces nouveaux axes\n\nils peuvent servir pour contr√¥ler la pertinence de la nouvelle base\n\n\nCes choix peuvent diff√©rer selon l‚Äôanalyse que nous souhaitons r√©aliser. Ici nous choisissons par exemple :\n\nvariables actives : les temp√©ratures de janvier √† d√©cembre\n\nles colonnes 13 √† 16 sont des variables quantitatives suppl√©mentaires\nla colonne 17 est une variable qualitative suppl√©mentaire\n\nindividus actifs : les capitales\n\nles autres (lignes 24 √† 35) seront suppl√©mentaires\n\n\n\nres_acp &lt;- PCA(temperature, \n               ind.sup = c(24:35),\n               quanti.sup = c(13:16),\n               quali.sup = c(17),\n               graph = FALSE)\n\nVoici les coordonn√©es des villes dans la nouvelle base :\n\nres_acp$ind$coord\n\n                 Dim.1        Dim.2       Dim.3        Dim.4        Dim.5\nAmsterdam   0.22693852 -1.371378702 -0.10439354 -0.282792759 -0.225234980\nAthenes     7.60067204  0.930375742  0.56142895 -0.286557237  0.121284086\nBerlin     -0.28785832  0.016454075 -0.29060835 -0.055593051 -0.141777442\nBruxelles   0.63117358 -1.177217640 -0.15204276  0.017069866 -0.118080300\nBudapest    1.66802839  1.712697730 -0.49898331  0.112436553  0.147493796\nCopenhague -1.46239513 -0.492056307  0.44036858 -0.176716151  0.001359796\nDublin     -0.50524137 -2.673496925 -0.17850939  0.029911177  0.199489106\nHelsinki   -4.03629712  0.462039367  0.59318909 -0.244444751  0.058755870\nKiev       -1.71222008  2.007597607 -0.17067034 -0.112807425  0.046761788\nCracovie   -1.25865727  0.874989077 -0.27396329  0.036537296 -0.022272155\nLisbonne    5.59928833 -1.554345838 -0.27035193 -0.137240393  0.057997452\nLondres     0.05764006 -1.573766723 -0.08467398  0.052014838  0.166383037\nMadrid      4.06406743  0.697664862  0.46179949  0.662933274 -0.076738822\nMinsk      -3.23789748  1.391289730 -0.07227015 -0.183341936  0.066896667\nMoscou     -3.46261171  2.182015808 -0.30130242 -0.005382798  0.075779200\nOslo       -3.30598698  0.310053024  0.29530945  0.190608834  0.068558517\nParis       1.41971350 -0.897598545 -0.11032749 -0.079067225 -0.184884193\nPrague     -0.10900287  0.698041163 -0.24257136  0.100375201  0.119810451\nReykjavik  -4.70406435 -2.957197699 -0.05789842  0.195366437  0.102746920\nRome        5.38200124  0.293698723  0.18926869 -0.012589110  0.077649640\nSarajevo    0.16345193  0.319489453 -0.36458614  0.073615022 -0.204664346\nSofia       0.41781097  0.795074460 -0.24086374  0.047803563 -0.195020427\nStockholm  -3.14855331  0.005577557  0.87265235  0.057860775 -0.142293659\n\n\n\n\n1.4 Inertie par axe\nLa dimension 1 est l‚Äôaxe avec la plus grande inertie, la dimension 2 avec la 2e plus grande inertie‚Ä¶\n\nfviz_eig(res_acp)\n\n\n\n\n\n\n\n\nLe premier axe explique 82.9 % de l‚Äôinertie et le second 15,4 %.\n\n\n1.5 Cercle des corr√©lations\nLe cercle des corr√©lations permet de repr√©senter graphiquement les corr√©lations entre les variables et les axes factoriels.\nRepr√©sentons ce cercle pour les 2 premiers axes :\n\nfviz_pca_var(res_acp, repel = TRUE)\n\n\n\n\n\n\n\n\n\nInterpr√©tations des fl√®ches. Si elle pointe pr√©s de\n\n[1,0], la variable est fortement corr√©l√©e positivement √† l‚Äôaxe factoriel des abscisses (1)\n[-1,0], la variable est fortement corr√©l√©e n√©gativement √† l‚Äôaxe factoriel des abscisses (1)\n[0,1], la variable est fortement corr√©l√©e positivement √† l‚Äôaxe factoriel des ordonn√©es (2)\n[0,-1], la variable est fortement corr√©l√©e n√©gativement √† l‚Äôaxe factoriel des ordonn√©es (2)\n[0,0], la variable n‚Äôest corr√©l√© √† aucun axe factoriel\n\n\nDans notre exemple, les 12 variables de temp√©ratures moyennes mensuelle sont fortement correl√©s positivement √† l‚Äôaxe 1. Ce comportement est nomm√© effet taille.\nC‚Äôest assez logique car cet axe repr√©sente une √©crasante majorit√© de l‚Äôinertie.\nAutre information interessante : la variable Amplitude est fortement correl√©e au 2e axe. Et pour tant cette variable est suppl√©mentaire et n‚Äôa donc pas √©t√© prise en compte pour construire cet axe.\n\n\n1.6 Individus sur les 2 premiers axes\nRepr√©sentons maintenant les individus sur les axes factoriels.\n\nfviz_pca_ind(res_acp, repel = TRUE)\n\n\n\n\n\n\n\n\nEssayons d‚Äôinterpr√©ter ces 2 axes :\n\naxe 1 : nous retrouvons des villes o√π les temp√©ratures sont plut√¥t froides √† gauche et chaudes √† droite\naxe 2 :\n\nen haut nous retrouvons des villes avec un climat continental (grandes variations de temp√©ratures entre les saisons)\nalors qu‚Äôen bas, nous retrouvons plut√¥t des villes avec un climat oc√©anique (temp√©ratures plus stables)\n\n\n\nfviz_pca_ind(res_acp,\n             habillage = \"Region\", \n             addEllipses = TRUE,\n             invisible = c(\"ind.sup\"))\n\n\n\n\n\n\n\n\nNous pouvons √©galement visualiser les individus sur les autres axes.\nDans le cas pr√©sent l‚Äôinertie des axes 3 et 4 est tr√®s faible, il est donc compliqu√© d‚Äôinterpr√©ter ces axes.\n\nfviz_pca_ind(res_acp,\n             repel = TRUE,\n             axes = c(3,4),\n             invisible = c(\"ind.sup\"))\n\n\n\n\n\n\n\n\n\n\n1.7 Contribution aux axes\nLa contribution aux axes d‚Äôune variable indique si la variable a fortement contribu√© pour la construction de cet axe.\n\nplus une variable contribue √† une composante principale, plus sa contribution aux axes sera √©lev√©e\ncette mesure permet d‚Äôidentifier les variables qui ont le plus d‚Äôimpact sur chaque composante principale\n\n\nfviz_contrib(res_acp, choice = \"var\", axes = 1)\n\n\n\n\n\n\n\n\nIl en va de m√™me pour la contribution aux axes d‚Äôun individu.\n\nfviz_contrib(res_acp, choice = \"ind\", axes = 2)\n\n\n\n\n\n\n\n\n\n\n1.8 Cos¬≤\nLe cos¬≤ mesure la qualit√© de repr√©sentation de chaque variable sur chaque axe.\n\nun cos¬≤ proche de 1 indique une bonne repr√©sentation de la variable sur l‚Äôaxe, ce qui signifie que la variable est bien align√©e avec l‚Äôaxe et contribue fortement √† sa d√©finition\nun cos¬≤ proche de 0 indique une faible repr√©sentation de la variable sur l‚Äôaxe, ce qui signifie que la variable n‚Äôest pas bien align√©e avec l‚Äôaxe et contribue peu √† sa d√©finition\n\n\nfviz_cos2(res_acp, choice = \"var\", axe = 1)\n\n\n\n\n\n\n\n\nIdem pour les individus.\n\nfviz_cos2(res_acp, choice = \"ind\", axe = 1)"
  },
  {
    "objectID": "doc/tuto/stats/analyse-multivariee.html#vision-math√©matique",
    "href": "doc/tuto/stats/analyse-multivariee.html#vision-math√©matique",
    "title": "Analyse multivari√©e",
    "section": "2 Vision math√©matique",
    "text": "2 Vision math√©matique\n\n2.1 Matrices des donn√©es\n\nX &lt;- as.matrix(temperature[1:23,1:12])\ncolnames(X) &lt;- c(\"jan\", \"fev\", \"mar\", \"avr\", \"mai\", \"jui\", \"jul\", \"aou\", \"sep\", \"oct\", \"nov\", \"dec\")\n\nX\n\n            jan  fev  mar  avr  mai  jui  jul  aou  sep  oct  nov  dec\nAmsterdam   2.9  2.5  5.7  8.2 12.5 14.8 17.1 17.1 14.5 11.4  7.0  4.4\nAthenes     9.1  9.7 11.7 15.4 20.1 24.5 27.4 27.2 23.8 19.2 14.6 11.0\nBerlin     -0.2  0.1  4.4  8.2 13.8 16.0 18.3 18.0 14.4 10.0  4.2  1.2\nBruxelles   3.3  3.3  6.7  8.9 12.8 15.6 17.8 17.8 15.0 11.1  6.7  4.4\nBudapest   -1.1  0.8  5.5 11.6 17.0 20.2 22.0 21.3 16.9 11.3  5.1  0.7\nCopenhague -0.4 -0.4  1.3  5.8 11.1 15.4 17.1 16.6 13.3  8.8  4.1  1.3\nDublin      4.8  5.0  5.9  7.8 10.4 13.3 15.0 14.6 12.7  9.7  6.7  5.4\nHelsinki   -5.8 -6.2 -2.7  3.1 10.2 14.0 17.2 14.9  9.7  5.2  0.1 -2.3\nKiev       -5.9 -5.0 -0.3  7.4 14.3 17.8 19.4 18.5 13.7  7.5  1.2 -3.6\nCracovie   -3.7 -2.0  1.9  7.9 13.2 16.9 18.4 17.6 13.7  8.6  2.6 -1.7\nLisbonne   10.5 11.3 12.8 14.5 16.7 19.4 21.5 21.9 20.4 17.4 13.7 11.1\nLondres     3.4  4.2  5.5  8.3 11.9 15.1 16.9 16.5 14.0 10.2  6.3  4.4\nMadrid      5.0  6.6  9.4 12.2 16.0 20.8 24.7 24.3 19.8 13.9  8.7  5.4\nMinsk      -6.9 -6.2 -1.9  5.4 12.4 15.9 17.4 16.3 11.6  5.8  0.1 -4.2\nMoscou     -9.3 -7.6 -2.0  6.0 13.0 16.6 18.3 16.7 11.2  5.1 -1.1 -6.0\nOslo       -4.3 -3.8 -0.6  4.4 10.3 14.9 16.9 15.4 11.1  5.7  0.5 -2.9\nParis       3.7  3.7  7.3  9.7 13.7 16.5 19.0 18.7 16.1 12.5  7.3  5.2\nPrague     -1.3  0.2  3.6  8.8 14.3 17.6 19.3 18.7 14.9  9.4  3.8  0.3\nReykjavik  -0.3  0.1  0.8  2.9  6.5  9.3 11.1 10.6  7.9  4.5  1.7  0.2\nRome        7.1  8.2 10.5 13.7 17.8 21.7 24.4 24.1 20.9 16.5 11.7  8.3\nSarajevo   -1.4  0.8  4.9  9.3 13.8 17.0 18.9 18.7 15.2 10.5  5.1  0.8\nSofia      -1.7  0.2  4.3  9.7 14.3 17.7 20.0 19.5 15.8 10.7  5.0  0.6\nStockholm  -3.5 -3.5 -1.3  3.5  9.2 14.6 17.2 16.0 11.7  6.5  1.7 -1.6\n\n\n\n\n2.2 Matrice des variances-covariances\nC‚Äôest une matrice carr√©e sym√©trique qui contient :\n\nles variances des variables sur la diagonale\nles covariances entre les paires de variables hors-diagonale\n\n\nFonction covCalcul √† la main\n\n\n\nmatrice_var_cov &lt;- cov(X); \n\nround(matrice_var_cov, 1)\n\n     jan  fev  mar  avr  mai  jui  jul  aou  sep  oct  nov  dec\njan 26.8 26.2 21.9 13.6  7.5  6.8  8.0 10.4 14.6 18.0 21.0 23.7\nfev 26.2 26.2 22.3 14.6  8.6  7.9  9.0 11.5 15.5 18.4 21.0 23.2\nmar 21.9 22.3 20.2 14.3  9.6  8.8  9.8 12.0 15.0 17.0 18.5 19.6\navr 13.6 14.6 14.3 12.1  9.7  9.3 10.0 11.4 12.7 13.1 13.1 12.6\nmai  7.5  8.6  9.6  9.7  9.2  9.2  9.7 10.4 10.5  9.9  8.9  7.5\njui  6.8  7.9  8.8  9.3  9.2  9.9 10.5 11.1 10.7  9.7  8.5  6.9\njul  8.0  9.0  9.8 10.0  9.7 10.5 11.6 12.1 11.7 10.6  9.5  8.0\naou 10.4 11.5 12.0 11.4 10.4 11.1 12.1 13.0 13.0 12.4 11.4 10.1\nsep 14.6 15.5 15.0 12.7 10.5 10.7 11.7 13.0 14.2 14.4 14.3 13.7\noct 18.0 18.4 17.0 13.1  9.9  9.7 10.6 12.4 14.4 15.7 16.3 16.6\nnov 21.0 21.0 18.5 13.1  8.9  8.5  9.5 11.4 14.3 16.3 17.9 19.1\ndec 23.7 23.2 19.6 12.6  7.5  6.9  8.0 10.1 13.7 16.6 19.1 21.3\n\n\n\n\n\n# On centre toutes les variables\ncentered_X &lt;- scale(X, center = TRUE, scale = FALSE)\n\nround(1/(nrow(centered_X)-1) * t(centered_X) %*% centered_X, 1)\n\n     jan  fev  mar  avr  mai  jui  jul  aou  sep  oct  nov  dec\njan 26.8 26.2 21.9 13.6  7.5  6.8  8.0 10.4 14.6 18.0 21.0 23.7\nfev 26.2 26.2 22.3 14.6  8.6  7.9  9.0 11.5 15.5 18.4 21.0 23.2\nmar 21.9 22.3 20.2 14.3  9.6  8.8  9.8 12.0 15.0 17.0 18.5 19.6\navr 13.6 14.6 14.3 12.1  9.7  9.3 10.0 11.4 12.7 13.1 13.1 12.6\nmai  7.5  8.6  9.6  9.7  9.2  9.2  9.7 10.4 10.5  9.9  8.9  7.5\njui  6.8  7.9  8.8  9.3  9.2  9.9 10.5 11.1 10.7  9.7  8.5  6.9\njul  8.0  9.0  9.8 10.0  9.7 10.5 11.6 12.1 11.7 10.6  9.5  8.0\naou 10.4 11.5 12.0 11.4 10.4 11.1 12.1 13.0 13.0 12.4 11.4 10.1\nsep 14.6 15.5 15.0 12.7 10.5 10.7 11.7 13.0 14.2 14.4 14.3 13.7\noct 18.0 18.4 17.0 13.1  9.9  9.7 10.6 12.4 14.4 15.7 16.3 16.6\nnov 21.0 21.0 18.5 13.1  8.9  8.5  9.5 11.4 14.3 16.3 17.9 19.1\ndec 23.7 23.2 19.6 12.6  7.5  6.9  8.0 10.1 13.7 16.6 19.1 21.3\n\n\n\n\n\n\n\n2.3 Matrice de corr√©lation\nC‚Äôest la matrice des variances-covariances normalis√©e.\n\nFontion corCalcul √† la main\n\n\n\nmatrice_cor &lt;- cor(X); \n\nround(matrice_cor, 2)\n\n     jan  fev  mar  avr  mai  jui  jul  aou  sep  oct  nov  dec\njan 1.00 0.99 0.94 0.75 0.48 0.42 0.45 0.56 0.75 0.88 0.96 0.99\nfev 0.99 1.00 0.97 0.82 0.56 0.49 0.52 0.62 0.80 0.91 0.97 0.98\nmar 0.94 0.97 1.00 0.92 0.70 0.62 0.64 0.74 0.89 0.96 0.97 0.95\navr 0.75 0.82 0.92 1.00 0.92 0.86 0.84 0.91 0.97 0.95 0.89 0.78\nmai 0.48 0.56 0.70 0.92 1.00 0.97 0.94 0.96 0.92 0.83 0.69 0.54\njui 0.42 0.49 0.62 0.86 0.97 1.00 0.99 0.98 0.90 0.78 0.64 0.47\njul 0.45 0.52 0.64 0.84 0.94 0.99 1.00 0.99 0.91 0.79 0.66 0.51\naou 0.56 0.62 0.74 0.91 0.96 0.98 0.99 1.00 0.96 0.87 0.75 0.61\nsep 0.75 0.80 0.89 0.97 0.92 0.90 0.91 0.96 1.00 0.97 0.90 0.79\noct 0.88 0.91 0.96 0.95 0.83 0.78 0.79 0.87 0.97 1.00 0.97 0.91\nnov 0.96 0.97 0.97 0.89 0.69 0.64 0.66 0.75 0.90 0.97 1.00 0.98\ndec 0.99 0.98 0.95 0.78 0.54 0.47 0.51 0.61 0.79 0.91 0.98 1.00\n\n\n\n\n\n# On centre et r√©duit toutes les variables\nnormalized_X &lt;- scale(X, center = TRUE, scale = TRUE)\n\nround(1/(nrow(normalized_X)-1) * t(normalized_X) %*% normalized_X, 2)\n\n     jan  fev  mar  avr  mai  jui  jul  aou  sep  oct  nov  dec\njan 1.00 0.99 0.94 0.75 0.48 0.42 0.45 0.56 0.75 0.88 0.96 0.99\nfev 0.99 1.00 0.97 0.82 0.56 0.49 0.52 0.62 0.80 0.91 0.97 0.98\nmar 0.94 0.97 1.00 0.92 0.70 0.62 0.64 0.74 0.89 0.96 0.97 0.95\navr 0.75 0.82 0.92 1.00 0.92 0.86 0.84 0.91 0.97 0.95 0.89 0.78\nmai 0.48 0.56 0.70 0.92 1.00 0.97 0.94 0.96 0.92 0.83 0.69 0.54\njui 0.42 0.49 0.62 0.86 0.97 1.00 0.99 0.98 0.90 0.78 0.64 0.47\njul 0.45 0.52 0.64 0.84 0.94 0.99 1.00 0.99 0.91 0.79 0.66 0.51\naou 0.56 0.62 0.74 0.91 0.96 0.98 0.99 1.00 0.96 0.87 0.75 0.61\nsep 0.75 0.80 0.89 0.97 0.92 0.90 0.91 0.96 1.00 0.97 0.90 0.79\noct 0.88 0.91 0.96 0.95 0.83 0.78 0.79 0.87 0.97 1.00 0.97 0.91\nnov 0.96 0.97 0.97 0.89 0.69 0.64 0.66 0.75 0.90 0.97 1.00 0.98\ndec 0.99 0.98 0.95 0.78 0.54 0.47 0.51 0.61 0.79 0.91 0.98 1.00\n\n\n\n\n\nComparaison avec les valeurs propres donn√©es par la fonction pca\n\ndata.frame(\n  \"PCA\" = as.vector(res_acp$eig[,\"eigenvalue\"]),\n  \"√† la main\" = eigen(cor(X))$values\n)\n\n\n\n\n\nPCA\n√†.la.main\n\n\n\n\n9.9477504\n9.9477504\n\n\n1.8476485\n1.8476485\n\n\n0.1262558\n0.1262558\n\n\n0.0382934\n0.0382934\n\n\n0.0167094\n0.0167094\n\n\n0.0128330\n0.0128330\n\n\n0.0058303\n0.0058303\n\n\n0.0020319\n0.0020319\n\n\n0.0010235\n0.0010235\n\n\n0.0009528\n0.0009528\n\n\n0.0005368\n0.0005368\n\n\n0.0001342\n0.0001342\n\n\n\n\n\n\nL‚Äôinertie du premier axe est √©gale √† :\n\\[\\frac{Valeur\\ propre\\ 1}{Somme\\ des\\ valeurs\\ propres}\\]\n\neigen(cor(X))$values[1] / sum(eigen(cor(X))$values)\n\n[1] 0.8289792"
  },
  {
    "objectID": "doc/tuto/stats/analyse-multivariee.html#afd",
    "href": "doc/tuto/stats/analyse-multivariee.html#afd",
    "title": "Analyse multivari√©e",
    "section": "3 AFD",
    "text": "3 AFD\nüöß\nUne Analyse Factorielle Discriminante (AFD) est une ACP √† laquelle nous ajoutons une variable qualitative."
  },
  {
    "objectID": "doc/tuto/stats/analyse-multivariee.html#afc",
    "href": "doc/tuto/stats/analyse-multivariee.html#afc",
    "title": "Analyse multivari√©e",
    "section": "4 AFC",
    "text": "4 AFC\nüöß\nUne Analyse Factorielle des Correspondances (AFC) eprmet d‚Äô√©tudier la liaison entre 2 variables qualitatives."
  },
  {
    "objectID": "doc/tuto/stats/analyse-multivariee.html#acm",
    "href": "doc/tuto/stats/analyse-multivariee.html#acm",
    "title": "Analyse multivari√©e",
    "section": "5 ACM",
    "text": "5 ACM\nüöß\nUne Analyse factorielle des correspondances multiples (ACM) g√©n√©ralise l‚ÄôAFC en √©tudiant les liaisons entre plusieurs variables qualitatives."
  },
  {
    "objectID": "doc/tuto/stats/analyse-multivariee.html#vid√©os",
    "href": "doc/tuto/stats/analyse-multivariee.html#vid√©os",
    "title": "Analyse multivari√©e",
    "section": "6 Vid√©os",
    "text": "6 Vid√©os\nStatQuest: ACP, id√©es principales :"
  },
  {
    "objectID": "doc/tuto/proba/lois-probabilite-continues.html",
    "href": "doc/tuto/proba/lois-probabilite-continues.html",
    "title": "Lois de probabilit√© continues",
    "section": "",
    "text": "Moyenne : \\[\\mu\\]\nVariance : \\[\\sigma^2\\]\nDensit√© : \\[f(t)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{1}{2} \\frac{(t-\\mu )^{2}}{\\sigma ^{2}}}\\]\nFonction caract√©ristique : \\[\\phi (t)={\\rm {e}}^{\\mu {\\rm {i}}t-{\\frac {1}{2}}\\sigma ^{2}t^{2}}\\]\nPropri√©t√©s : \\[X_1 + X_2 \\sim \\mathcal{N}(\\mu_1 + \\mu_2,\\sigma_1^2 + \\sigma_2^2) \\Leftrightarrow {\\begin{cases}X_1 \\sim \\mathcal{N}(\\mu_1,\\sigma_1^2) \\\\ X_2 \\sim \\mathcal{N}(\\mu_2,\\sigma_2^2) \\\\ X \\perp \\!\\!\\! \\perp Y \\end{cases}}\\]\n\nDensit√©Fonction de r√©partitionG√©n√©rer un √©chantillon\n\n\n\n\nCode\nlibrary(ggplot2)\n\n# Param√®tres\nmu &lt;- 0\nsigma2 &lt;- 1\n\n# G√©n√©rer des donn√©es\nx &lt;- seq(mu - 3, mu + 3, length.out = 100)\nf_x &lt;- dnorm(x, mean = mu, sd = sigma2)\ndata &lt;- data.frame(x = x, density = f_x)\n\n# Tracer la courbe avec ggplot2\nggplot(data, aes(x = x, y = density)) +\n  geom_line(color = \"darkcyan\", linewidth = 1.5) +\n  scale_x_continuous(name = \"x\", limits = c(-3, 3)) + \n  scale_y_continuous(name = \"f(x)\") + \n  labs(title = \"Densit√© de la loi Normale(0, 1)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\n\n# Param√®tres\na &lt;- 0\nb &lt;- 1\n\n# G√©n√©rer des donn√©es\nx &lt;- seq(mu - 3, mu + 3, length.out = 100)\nF_x &lt;- pnorm(x, mean = mu, sd = sigma2)\ndata &lt;- data.frame(x = x, cdf = F_x)\n\n# Tracer la courbe avec ggplot2\nggplot(data, aes(x = x, y = cdf)) +\n  geom_line(color = \"darkcyan\", linewidth = 1.5) +\n  scale_x_continuous(name = \"x\", limits = c(-3, 3)) + \n  scale_y_continuous(name = \"F(x)\") + \n  labs(title = \"Fonction de r√©partition de la loi Normale(0, 1)\")\n\n\n\n\n\n\n\n\n\n\n\nG√©n√©rer un √©chantillon de n valeurs suivant la loi \\(\\mathcal{N}[0,1]\\)\n\nn &lt;- 5\nech &lt;- rnorm(n, 0, 1)\ndata.frame(ech)"
  },
  {
    "objectID": "doc/tuto/proba/lois-probabilite-continues.html#loi-normale",
    "href": "doc/tuto/proba/lois-probabilite-continues.html#loi-normale",
    "title": "Lois de probabilit√© continues",
    "section": "",
    "text": "Moyenne : \\[\\mu\\]\nVariance : \\[\\sigma^2\\]\nDensit√© : \\[f(t)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{1}{2} \\frac{(t-\\mu )^{2}}{\\sigma ^{2}}}\\]\nFonction caract√©ristique : \\[\\phi (t)={\\rm {e}}^{\\mu {\\rm {i}}t-{\\frac {1}{2}}\\sigma ^{2}t^{2}}\\]\nPropri√©t√©s : \\[X_1 + X_2 \\sim \\mathcal{N}(\\mu_1 + \\mu_2,\\sigma_1^2 + \\sigma_2^2) \\Leftrightarrow {\\begin{cases}X_1 \\sim \\mathcal{N}(\\mu_1,\\sigma_1^2) \\\\ X_2 \\sim \\mathcal{N}(\\mu_2,\\sigma_2^2) \\\\ X \\perp \\!\\!\\! \\perp Y \\end{cases}}\\]\n\nDensit√©Fonction de r√©partitionG√©n√©rer un √©chantillon\n\n\n\n\nCode\nlibrary(ggplot2)\n\n# Param√®tres\nmu &lt;- 0\nsigma2 &lt;- 1\n\n# G√©n√©rer des donn√©es\nx &lt;- seq(mu - 3, mu + 3, length.out = 100)\nf_x &lt;- dnorm(x, mean = mu, sd = sigma2)\ndata &lt;- data.frame(x = x, density = f_x)\n\n# Tracer la courbe avec ggplot2\nggplot(data, aes(x = x, y = density)) +\n  geom_line(color = \"darkcyan\", linewidth = 1.5) +\n  scale_x_continuous(name = \"x\", limits = c(-3, 3)) + \n  scale_y_continuous(name = \"f(x)\") + \n  labs(title = \"Densit√© de la loi Normale(0, 1)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\n\n# Param√®tres\na &lt;- 0\nb &lt;- 1\n\n# G√©n√©rer des donn√©es\nx &lt;- seq(mu - 3, mu + 3, length.out = 100)\nF_x &lt;- pnorm(x, mean = mu, sd = sigma2)\ndata &lt;- data.frame(x = x, cdf = F_x)\n\n# Tracer la courbe avec ggplot2\nggplot(data, aes(x = x, y = cdf)) +\n  geom_line(color = \"darkcyan\", linewidth = 1.5) +\n  scale_x_continuous(name = \"x\", limits = c(-3, 3)) + \n  scale_y_continuous(name = \"F(x)\") + \n  labs(title = \"Fonction de r√©partition de la loi Normale(0, 1)\")\n\n\n\n\n\n\n\n\n\n\n\nG√©n√©rer un √©chantillon de n valeurs suivant la loi \\(\\mathcal{N}[0,1]\\)\n\nn &lt;- 5\nech &lt;- rnorm(n, 0, 1)\ndata.frame(ech)"
  },
  {
    "objectID": "doc/tuto/proba/lois-probabilite-continues.html#loi-uniforme",
    "href": "doc/tuto/proba/lois-probabilite-continues.html#loi-uniforme",
    "title": "Lois de probabilit√© continues",
    "section": "2 Loi Uniforme",
    "text": "2 Loi Uniforme\nLoi uniforme continue sur le segment [a,b].\nEsp√©rance : \\[\\mathbb{E}( \\mathcal{U}[a,b]) = \\frac{a+b}{2}\\]\nVariance : \\[\\mathbb{V}( \\mathcal{U}[a,b]) = \\frac{(b-a)^2}{12}\\]\nDensit√© : \\[f(x)=\\frac{1}{b-a} \\mathbb{1}_{a \\leq x \\leq b}(x)\\]\nFonction de r√©partition : \\[F(x)={\\begin{cases}0&{\\text{pour }}x&lt;a\\\\{\\dfrac {x-a}{b-a}}&{\\text{pour }}a\\leq x&lt;b\\\\1&{\\text{pour }}x\\geq b\\end{cases}}\\]\nFonction caract√©ristique : \\[\\phi(t) = \\frac {{\\rm {e}}^{{\\rm {i}}tb}-{\\rm {e}}^{{\\rm {i}}ta}}{{\\rm {i}}t(b-a)}\\]\nCode R\n\nDensit√©Fonction de r√©partitionG√©n√©rer un √©chantillon\n\n\n\n\nCode\nlibrary(ggplot2)\n\n# Param√®tres\na &lt;- 0\nb &lt;- 1\n\n# G√©n√©rer des donn√©es\nx &lt;- seq(a - 0.1, b + 0.1, length.out = 100)\nf_x &lt;- dunif(x, min = a, max = b)\ndata &lt;- data.frame(x = x, density = f_x)\n\n# Tracer la courbe avec ggplot2\nggplot(data, aes(x = x, y = density)) +\n  geom_line(color = \"darkcyan\", linewidth = 1.5) +\n  scale_x_continuous(name = \"x\", breaks = seq(0, 1, by = 0.2), limits = c(-0.1, 1.1)) + \n  scale_y_continuous(name = \"f(x)\") + \n  labs(title = \"Densit√© de la loi Uniforme\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\n\n# Param√®tres\na &lt;- 0\nb &lt;- 1\n\n# G√©n√©rer des donn√©es\nx &lt;- seq(a - 0.1, b + 0.1, length.out = 100)\nF_x &lt;- punif(x, min = a, max = b)\ndata &lt;- data.frame(x = x, cdf = F_x)\n\n# Tracer la courbe avec ggplot2\nggplot(data, aes(x = x, y = cdf)) +\n  geom_line(color = \"darkcyan\", linewidth = 1.5) +\n  scale_x_continuous(name = \"x\", breaks = seq(0, 1, by = 0.2), limits = c(-0.1, 1.1)) + \n  scale_y_continuous(name = \"F(x)\") + \n  labs(title = \"Fonction de r√©partition de la loi Uniforme\")\n\n\n\n\n\n\n\n\n\n\n\nG√©n√©rer un √©chantillon de n valeurs suivant la loi \\(\\mathcal{U}[a,b]\\)\n\nn &lt;- 5\nech &lt;- runif(n, min = 0, max = 1)\ndata.frame(ech)"
  },
  {
    "objectID": "doc/tuto/proba/lois-probabilite-continues.html#loi-du-chi2",
    "href": "doc/tuto/proba/lois-probabilite-continues.html#loi-du-chi2",
    "title": "Lois de probabilit√© continues",
    "section": "3 Loi du Chi2",
    "text": "3 Loi du Chi2\nLoi du chi 2 √† k degr√©s de libert√©\n\\[\\chi^2(k) \\sim \\sum_{i=1}^{k} X_{i}^{2} \\ avec \\ X_i \\ \\sim \\mathcal{N}(0,1) \\ iid\\]\nEsp√©rance : \\[\\mathbb{E}(\\chi^2(k)) = n\\]\nVariance : \\[\\mathbb{V}(\\chi^2(k)) = 2n\\]\nDensit√© : \\[\\frac{(1/2)^{{k/2}}}{\\Gamma (k/2)} x^{{k/2-1}} e^{{-x/2}}\\]\nFonction caract√©ristique : \\[\\phi(t) = (1-2\\,i\\,t)^{{-k/2}}\\]\nCode R\n\nDensit√©G√©n√©rer un √©chantillon\n\n\n\n\nCode\nlibrary(ggplot2)\n\n# Param√®tres\ndf_values &lt;- c(1, 2, 3, 5, 10, 20)\n\n# Gen√©rer des donn√©es\ndata &lt;- lapply(df_values, function(df) {\n  x &lt;- seq(0, 30, length.out = 200)\n  f_x &lt;- dchisq(x, df = df)\n  data.frame(x = x, density = f_x, df = as.factor(df))\n})\ndata &lt;- do.call(rbind, data)\n\n# Tracer la courbe avec ggplot2\nggplot(data, aes(x = x, y = density, color = df)) +\n  geom_line(linewidth = 1) +\n  scale_x_continuous(name = \"x\", limits = c(0, 30)) + \n  scale_y_continuous(name = \"f(x)\") + \n  labs(title = \"Densit√© de la loi du Chi2 selon diff√©rents degr√©s de libert√©\") +\n  scale_color_discrete(name = \"df\")\n\n\n\n\n\n\n\n\n\n\n\nG√©n√©rer un √©chantillon de n valeurs suivant la loi \\(\\chi^2(df)\\)\n\nn &lt;- 5\ndf &lt;- 3 \nech &lt;- rchisq(n, df)\ndata.frame(ech)"
  },
  {
    "objectID": "doc/tuto/proba/lois-probabilite-continues.html#loi-de-student",
    "href": "doc/tuto/proba/lois-probabilite-continues.html#loi-de-student",
    "title": "Lois de probabilit√© continues",
    "section": "4 Loi de Student",
    "text": "4 Loi de Student\nLoi de Student √† n degr√©s de libert√©.\n\\[\\mathcal{T}(n) \\sim \\frac{\\mathcal{N}(0,1)}{\\sqrt{\\chi^2(n)/n}}\\]\nEsp√©rance : \\[\\mathbb{E}(\\mathcal{T}(n)) = 0 \\ si \\ n &gt; 1\\]\nVariance : \\[\\mathbb{V}(\\mathcal{T}(n)) = \\frac{n}{n-2} \\ si \\ n &gt; 2 \\ (+\\infty \\ sinon)\\]\nLa loi de Student converge en loi vers la loi Normale \\[\\mathcal{T}(n) \\underset{n \\to +\\infty}{\\overset{\\mathcal{L}} \\longrightarrow} \\mathcal{N}(0,1)\\]\nCode R\n\nDensit√©G√©n√©rer un √©chantillon\n\n\n\n\nCode\nlibrary(ggplot2)\n\n# Param√®tres\ndf_values &lt;- c(1, 2, 5, 20)\n\n# G√©n√©rer des donn√©es\ndata &lt;- lapply(df_values, function(df) {\n  x &lt;- seq(-5, 5, length.out = 200)\n  f_x &lt;- dt(x, df = df)\n  data.frame(x = x, density = f_x, df = as.factor(df))\n})\ndata &lt;- do.call(rbind, data)\n\n# Tracer la courbe avec ggplot2\nggplot(data, aes(x = x, y = density, color = df)) +\n  geom_line(linewidth = 1) +\n  scale_x_continuous(name = \"x\", limits = c(-5, 5)) + \n  scale_y_continuous(name = \"f(x)\") + \n  labs(title = \"Densit√© de la loi de Student selon diff√©rents degr√©s de libert√©\") +\n  scale_color_discrete(name = \"df\")\n\n\n\n\n\n\n\n\n\n\n\nG√©n√©rer un √©chantillon de n valeurs suivant la loi \\(\\mathcal{T}(df)\\)\n\nn &lt;- 5\ndf &lt;- 3 \nech &lt;- rt(n, df)\ndata.frame(ech)"
  },
  {
    "objectID": "doc/tuto/proba/lois-probabilite-continues.html#loi-de-fisher-snedecor",
    "href": "doc/tuto/proba/lois-probabilite-continues.html#loi-de-fisher-snedecor",
    "title": "Lois de probabilit√© continues",
    "section": "5 Loi de Fisher-Snedecor",
    "text": "5 Loi de Fisher-Snedecor\nLoi de Fisher-Snedecor √† n et m degr√©s de libert√©\n\\(\\mathcal{F}(n,m) \\sim \\frac{\\chi^2(n)/n}{\\chi^2(m)/m}\\)\n\\(\\mathbb{E}(\\mathcal{F}(n,m)) = \\frac {m}{m-2}\\) si \\(m &gt; 2\\)\n\\(X \\sim \\mathcal{F}(a,b) \\Rightarrow \\frac{1}{X} \\sim \\mathcal{F}(b,a)\\)\nLien avec la loi de Student : \\(X\\sim \\mathcal{T}(n) \\Rightarrow X^2 \\sim \\mathcal{F}(1,n)\\)\nLien avec la loi Normale : \\(X \\sim \\mathcal{N}(0,1) \\Rightarrow X^2 \\sim \\mathcal{F}(1,\\infty)\\)"
  },
  {
    "objectID": "doc/tuto/proba/lois-probabilite-continues.html#loi-exponentielle",
    "href": "doc/tuto/proba/lois-probabilite-continues.html#loi-exponentielle",
    "title": "Lois de probabilit√© continues",
    "section": "6 Loi Exponentielle",
    "text": "6 Loi Exponentielle\nLoi exponentielle de param√®tre Lambda\n\\(\\mathbb{E}(\\epsilon(\\lambda)) = \\frac{1}{\\lambda}\\)\n\\(\\mathbb{V}(\\epsilon(\\lambda)) = \\frac{1}{\\lambda^2}\\)\nDensit√© : \\(\\lambda e^{{-\\lambda x}} \\mathbb{1}_{x \\geq 0}\\)\nFonction de r√©partition : \\(1-e^{{-\\lambda x}}\\)\nFonction caract√©ristique : \\(\\left(1-{\\dfrac {it}{\\lambda }}\\right)^{{-1}}\\)"
  },
  {
    "objectID": "doc/tuto/proba/lois-probabilite-continues.html#loi-gamma",
    "href": "doc/tuto/proba/lois-probabilite-continues.html#loi-gamma",
    "title": "Lois de probabilit√© continues",
    "section": "7 Loi Gamma",
    "text": "7 Loi Gamma\nLoi Gamma de param√®tres alpha et beta\n\\(X \\sim \\Gamma( \\alpha , \\beta)\\)\n\\(\\mathbb{E}(\\Gamma( \\alpha , \\beta)) = \\frac{\\alpha}{\\beta}\\)\n\\(\\mathbb{V}(\\Gamma( \\alpha , \\beta)) = \\frac{\\alpha}{\\beta^2}\\)\nDensit√© : \\(f ( x , \\alpha, \\beta ) = x^{\\alpha -1} \\frac{\\beta ^\\alpha e^{-\\beta x}}{\\Gamma (\\alpha )} \\mathbb{1}_{x &gt; 0}\\)\nLiens avec d‚Äôautres lois :\n\nSi \\(\\alpha = 1 \\sim\\) loi exponentielle\nSi \\(\\Gamma (n / 2 , 1/2 ) \\sim\\) loi du \\(\\chi^2\\) √† \\(n\\) degr√©s de libert√©\n\n\n7.1 Fonction Gamma\n\\(\\Gamma(a) = \\int _{0}^{+\\infty }t^{a-1}\\,\\mathrm {e} ^{-t}\\,\\mathrm dt\\)\nProlonge la fonction factorielle √† l‚Äôensemble des nombres complexes (sauf entiers n√©gatifs)\n\\(\\Gamma(n+1) = n!\\)\n\\(\\Gamma(x+1) = x\\Gamma(x)\\)\n\n\n7.2 Loi inverse Gamma\nLoi inverse Gamma de param√®tres k et \\(\\theta\\)\n\\(X \\sim {{Inv \\ \\Gamma}(k,\\theta )} \\Rightarrow \\frac{1}{X} \\sim \\Gamma ( k , 1 / \\theta )\\)\nDensit√© : \\(f(x;\\alpha ,\\beta )={\\frac {\\beta ^{\\alpha }}{\\Gamma (\\alpha )}}(1/x)^{\\alpha +1}\\exp \\left(-\\beta /x\\right)\\) pour \\(x &gt; 0\\)\n\\(\\mathbb{E}(Inv \\ \\Gamma( \\alpha , \\beta)) = \\frac{\\beta}{\\alpha-1}\\) pour \\(\\alpha &gt; 1\\)\n\\(\\mathbb{V}(Inv \\ \\Gamma( \\alpha , \\beta)) = \\frac {\\beta ^{2}}{(\\alpha -1)^{2}(\\alpha -2)}\\) pour \\(\\alpha &gt; 2\\)"
  },
  {
    "objectID": "doc/tuto/proba/lois-probabilite-continues.html#loi-beta",
    "href": "doc/tuto/proba/lois-probabilite-continues.html#loi-beta",
    "title": "Lois de probabilit√© continues",
    "section": "8 Loi Beta",
    "text": "8 Loi Beta\nLoi Beta de param√®tres alpha et beta\n\\(\\mathbb{E}(\\mathrm{B}) = \\frac {\\alpha }{\\alpha +\\beta }\\)\n\\(\\mathbb{V}(\\mathrm{B}) = \\frac {\\alpha \\beta }{(\\alpha +\\beta )^{2}(\\alpha +\\beta +1)}\\)\nDensit√© : \\(\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{\\mathrm{B} (\\alpha ,\\beta )}\\)\n\n8.1 Fonction Beta\n\\(\\mathrm {B} (x,y)=\\int _{0}^{1}t^{x-1}(1-t)^{y-1}\\mathrm {d} t\\)\n\\(\\mathrm {B} (x,y)={\\frac {\\Gamma (x)\\,\\Gamma (y)}{\\Gamma (x+y)}}\\)\n\\(\\mathrm {B} (x,y)=\\mathrm {B} (y,x)\\)\n\\(\\mathrm {B} (x,y+1)=\\frac{y}{x+y}\\mathrm {B} (x,y)\\)"
  },
  {
    "objectID": "doc/tuto/proba/probabilites.html",
    "href": "doc/tuto/proba/probabilites.html",
    "title": "Probabilit√©",
    "section": "",
    "text": "\\[\\mathbb{P}(X \\leq x) = \\mathbb{E}(\\mathbb{1}_{X \\leq x})\\] \\[\\mathbb{V}(X) = \\mathbb{E}((X-\\mathbb{E}(X))^2)\\]"
  },
  {
    "objectID": "doc/tuto/proba/probabilites.html#formules-de-base",
    "href": "doc/tuto/proba/probabilites.html#formules-de-base",
    "title": "Probabilit√©",
    "section": "",
    "text": "\\[\\mathbb{P}(X \\leq x) = \\mathbb{E}(\\mathbb{1}_{X \\leq x})\\] \\[\\mathbb{V}(X) = \\mathbb{E}((X-\\mathbb{E}(X))^2)\\]"
  },
  {
    "objectID": "doc/tuto/proba/probabilites.html#convergences",
    "href": "doc/tuto/proba/probabilites.html#convergences",
    "title": "Probabilit√©",
    "section": "2 Convergences",
    "text": "2 Convergences\n\nConvergence presque surement\n\n\\(X_n \\underset{n \\to +\\infty}{\\overset{p.s} \\longrightarrow} X \\Longleftrightarrow \\forall \\omega \\in \\Omega, \\lim\\limits_{n \\rightarrow +\\infty} X_n(\\omega) = X(\\omega)\\)\n\nConvergence en probabilit√©\n\n\\(X_n \\underset{n \\to +\\infty}{\\overset{\\mathbb{P}} \\longrightarrow} X \\Longleftrightarrow \\forall \\epsilon, \\lim\\limits_{n \\rightarrow +\\infty} \\mathbb{P}(||X_n-X||&gt;\\epsilon) = 0\\)\n\nConvergence dans \\(\\mathbb{L}^p\\)\n\n\\(X_n \\underset{n \\to +\\infty}{\\overset{\\mathbb{L}^p} \\longrightarrow} X \\Longleftrightarrow \\lim\\limits_{n \\rightarrow +\\infty} ||X_n-X||_p = 0\\)\n\nConvergence en Loi\n\n\\(X_n \\underset{n \\to +\\infty}{\\overset{\\mathcal{L}} \\longrightarrow} X \\Longleftrightarrow \\lim\\limits_{n \\rightarrow +\\infty} \\mathbb{E}(h(X_n)) = \\mathbb{E}(h(X)), \\forall h\\) continue born√©e\n\n\nImplications de convergences :\n\n\\(p.s \\Rightarrow \\mathbb{P}\\)\n\\(\\mathbb{L}^p \\Rightarrow \\mathbb{P}\\)\n\\(\\mathbb{P} \\Rightarrow \\mathcal{L}\\)"
  },
  {
    "objectID": "doc/tuto/proba/probabilites.html#loi-forte-des-grands-nombres",
    "href": "doc/tuto/proba/probabilites.html#loi-forte-des-grands-nombres",
    "title": "Probabilit√©",
    "section": "3 Loi forte des Grands Nombres",
    "text": "3 Loi forte des Grands Nombres\n\n3.1 √ânonc√©\nSoit \\((X_n)_{n\\geq1}\\) une suite de variables al√©atoires ind√©pendantes et identiquement distribu√©es (iid) et int√©grables √† valeurs dans \\(\\mathbb{R}^d\\). Alors :\n\\[\\frac{1}{n} \\sum_{i=1}^{n} X_i \\underset{n \\to +\\infty}{\\overset{\\text{p.s.}}{\\longrightarrow}} \\mathbb{E}(X_1)\\]\nFormulation simple : la moyenne empirique tend vers l‚Äôesp√©rance.\n\n\n3.2 Exemple\n\nLan√ßons un d√© non truqu√© 500 fois et calculons la moyenne obtenue apr√©s chaque lancer\nPlus le nombre de lancers augmente, plus la moyenne converge vers \\(3.5\\)\n\n\n\nCode\nlibrary(ggplot2)\n\n# Simuler les lancers de d√©s\nset.seed(200)\nn &lt;- 500\nech &lt;- sample(x = 1:6, size = n, replace = TRUE)\ndata &lt;- data.frame(x = 1:n, \n                   p = sapply(1:n, \n                              function(i) mean(ech[1:i])))\n\n# Tracer le graphique avec ggplot2\nggplot(data, aes(x = x, y = p)) +\n  geom_hline(yintercept = 3.5, \n             color = \"orange\", \n             linetype = \"dashed\",\n             linewidth = 1.1) +\n  geom_line(color = \"darkcyan\", linewidth = 1.2) +\n  labs(title = \"Moyenne des lancers de d√©\",\n       x = \"Nombre de lancers\",\n       y = \"Moyenne\") +\n  xlim(0, n) +\n  ylim(0, 6)"
  },
  {
    "objectID": "doc/tuto/proba/probabilites.html#th√©or√®me-central-limite",
    "href": "doc/tuto/proba/probabilites.html#th√©or√®me-central-limite",
    "title": "Probabilit√©",
    "section": "4 Th√©or√®me Central Limite",
    "text": "4 Th√©or√®me Central Limite\n\n4.1 √ânonc√©\nSoit \\((X_n)_{n\\geq1}\\) une suite de variables al√©atoires iid avec une moyenne \\(\\mu\\) et un √©cart-type \\(\\sigma\\). Alors :\n\\[\\lim_{n \\to +\\infty} \\sqrt{n} \\left( \\frac{1}{n}\\sum_{i=1}^{n} \\frac{ X_i - \\mu}{\\sigma} \\right) \\xrightarrow{\\mathcal {L}} N(0, 1)\\]\nFormulation simple : La diff√©rence entre moyenne empirique et esp√©rance (\\(\\overline{X}_n - \\mathbb{E}(X)\\)) converge en loi vers une loi Normale\n\n\n4.2 Exemple\n\nTirons un √©chantillon de 50 valeurs suivant la loi uniforme sur [0,1] et calculons la moyenne\nR√©p√©tons cette op√©ration 5000 fois et affichons la distribution des moyennes empiriques obtenues\n\n\n\nCode\nlibrary(ggplot2)\n\n# Param√®tres\nset.seed(200)\nnb_sim &lt;- 5000\ntaille_ech &lt;- 50\nu_min &lt;- 0\nu_max &lt;- 1\nE &lt;- (u_max - u_min) / 2\nV &lt;- (u_max - u_min)^2 / 12\n\n# Simulations\nres &lt;- numeric(nb_sim)\nfor (i in 1:nb_sim) {\n  ech &lt;- runif(taille_ech, min = u_min, max = u_max)\n  res[i] &lt;- sqrt(taille_ech) / sqrt(V) * (mean(ech) - E)\n}\n\n# Trac√© de l'histogramme et de la densit√©\nggplot(data.frame(res), aes(x = res)) +\n  geom_histogram(binwidth = 0.2, \n                 fill = \"darkcyan\", \n                 color = \"black\", \n                 aes(y = after_stat(density))) +\n  stat_function(aes(linetype = \"N(0,1)\"), \n                fun = dnorm, \n                args = list(mean = 0, sd = 1), \n                color = \"orange\", \n                linewidth = 1.5) +\n  labs(title = \"Distribution des moyennes empiriques\",\n       x = \"Moyenne des √©chantillons\",\n       y = \"Densit√©\") +\n  scale_linetype_manual(name = \"\", \n                        values = \"dashed\",\n                        labels = c(\"N(0,1)\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n4.3 Vid√©o\nStatQuest by Joshua Starmer"
  },
  {
    "objectID": "doc/tuto/proba/lois-probabilite-discretes.html",
    "href": "doc/tuto/proba/lois-probabilite-discretes.html",
    "title": "Lois de probabilit√© discr√®tes",
    "section": "",
    "text": "1 Loi de Poisson"
  },
  {
    "objectID": "doc/tuto/stats/regression-lineaire.html",
    "href": "doc/tuto/stats/regression-lineaire.html",
    "title": "R√©gression lin√©aire",
    "section": "",
    "text": "üöß"
  },
  {
    "objectID": "doc/tuto/stats/regression-lineaire.html#avant-de-commencer",
    "href": "doc/tuto/stats/regression-lineaire.html#avant-de-commencer",
    "title": "R√©gression lin√©aire",
    "section": "Avant de commencer",
    "text": "Avant de commencer\nR√©initialisation de l‚Äôenvironnement\n\nrm(list=ls())\n\nLibrairies utiles\n\nlibrary(dplyr)\nlibrary(ggplot2)"
  },
  {
    "objectID": "doc/tuto/stats/regression-lineaire.html#objectif",
    "href": "doc/tuto/stats/regression-lineaire.html#objectif",
    "title": "R√©gression lin√©aire",
    "section": "1 Objectif",
    "text": "1 Objectif\nUne r√©gression lin√©aire est une m√©thode statistique utilis√©e pour mod√©liser la relation entre :\n\nune variable √† expliquer not√©e \\(Y\\)\nune ou plusieurs variables explicatives lin√©airement ind√©pendantes (r√©gresseurs), not√©es \\(X_i\\)\n\nNotons \\(p\\) le nombre de regresseurs.\n\n\n\n\n\n\nExemple\n\n\n\nD√©finir un mod√®le pour d√©terminer le prix d‚Äôune maison avec 3 r√©gresseurs :\n\n\\(Y\\) : Prix\n\\(X_1\\) : Surface\n\\(X_2\\) : Nombre de toilettes\n\\(X_3\\) : Ann√©e de construction\n\n\n\nNous disposons d‚Äôun √©chantillon de taille \\(n\\) pour construire notre mod√®le :\n\n\n\n\n\n\nExemple\n\n\n\n\n\n\nY\nX1\nX2\nX3\n\n\n\n\n250 000\n100\n1\n1990\n\n\n350 000\n150\n2\n2000\n\n\n400 000\n180\n3\n2010\n\n\n300 000\n120\n2\n1985\n\n\n280 000\n110\n2\n2005\n\n\n\n\n\n√Ä partir de ces donn√©es, nous souhaitons trouver une fonction \\(f\\) telle que : \\(Y = f(X) + \\epsilon\\).\nDans le cadre de la r√©gression lin√©aire, la fonction \\(f\\) s‚Äôexprime simplement : \\(f(X) = X\\beta\\)\nNotre mod√®le devient alors : \\(Y = X\\beta + \\epsilon\\) avec :\n\n\\(\\beta\\) : le coefficient de r√©gression (pente de la droite)\n\\(\\epsilon\\) : le terme d‚Äôerreur\n\n\n\n\n\n\n\nLe r√©sidu\n\n\n\n\\(\\epsilon\\) est le terme d‚Äôerreur (ou r√©sidu). Il repr√©sente tout ce qui n‚Äôest pas expliqu√© par \\(X\\).\nEn d‚Äôautres termes, il capture la diff√©rence entre la valeur observ√©e de \\(Y\\) et la valeur pr√©dite par le mod√®le.\nLe terme \\(E\\) refl√®te les variations impr√©visibles ou dues √† d‚Äôautres facteurs non inclus dans le mod√®le.\nNous consid√©rerons le mod√®le gaussien, ainsi \\(\\epsilon ‚àº N (0, \\sigma^2 Id)\\).\n\n\nL‚Äô√©criture matricielle du mod√®le :\n\\[\\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ y_5 \\end{pmatrix} = \\begin{pmatrix} 1 & x_{11} & x_{12} & x_{13} \\\\ 1 & x_{21} & x_{22} & x_{23} \\\\ 1 & x_{31} & x_{32} & x_{33} \\\\ 1 & x_{41} & x_{42} & x_{43} \\\\ 1 & x_{51} & x_{52} & x_{53} \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{pmatrix} + \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\\\ \\epsilon_5 \\end{pmatrix}\\]\nCe qui √©quivaut √† \\(\\forall i = 1,...,n, Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}  + \\beta_3 x_{i3} + \\beta_4 x_{i4}\\)\n\n\n\n\n\n\nRemarque\n\n\n\n\nLa premi√®re colonne de \\(X\\) est une colonnes de 1 pour l‚Äôintercept\nLa seconde colonne de \\(X\\) est \\(X_1\\), la 3e est \\(X_2\\), la 4e est \\(X_3\\)\n\n\n\nL‚Äô√©tape suivante est d‚Äôestimer les coefficients du mod√®le. En utilisant la m√©thode des moindres carr√©s ordinaires (MCO), nous recherchons les valeurs des coefficients \\(\\beta\\) qui minimisent la somme des carr√©s des r√©sidus.\nL‚Äôestimateur MCO des coefficients \\(\\beta\\) est donn√© par : \\(\\hat{\\beta} = (X^T X)^{-1} X^T Y\\) sous les hypoth√®ses suivantes sur le terme d‚Äôerreur :\n\n\\(\\mathbb{E}(\\epsilon_i) = 0 \\ \\forall i = 1,...,n\\) (centr√©)\n\\(\\mathbb{V}(\\epsilon_i^2) = \\sigma^2 \\ \\forall i = 1,...,n\\) (homosc√©dastique)\n\\(Cov(\\epsilon_i, \\epsilon_j) = 0, \\ 1 \\le i \\ne j \\le n\\) (d√©corr√©l√©)"
  },
  {
    "objectID": "doc/tuto/stats/regression-lineaire.html#premier-exemple",
    "href": "doc/tuto/stats/regression-lineaire.html#premier-exemple",
    "title": "R√©gression lin√©aire",
    "section": "2 Premier exemple",
    "text": "2 Premier exemple\n\nn &lt;- 200    # Taille de l echantillon\np &lt;- 3      # Nombre de regresseurs\n\n\n2.1 G√©n√©rerons des donn√©es\n\nset.seed(234)\n\nX1 &lt;- runif(n, min=0, max=1)\nX2 &lt;- seq(1,n)\nX3 &lt;- X1^2 + 1\nE &lt;- rnorm(n, mean=0, sd=1)\nY &lt;- 15 + 3*X1 + X2/10 + X3/2 + E\ndf &lt;- data.frame(X1, X2, X3, Y)\n\nhead(df, 10)\n\n\n\n\n\nX1\nX2\nX3\nY\n\n\n\n\n0.7456200\n1\n1.555949\n18.30523\n\n\n0.7817124\n2\n1.611074\n19.07200\n\n\n0.0200371\n3\n1.000401\n14.91214\n\n\n0.7760854\n4\n1.602308\n18.87371\n\n\n0.0669101\n5\n1.004477\n13.75483\n\n\n0.6447951\n6\n1.415761\n17.99670\n\n\n0.9293860\n7\n1.863758\n19.79870\n\n\n0.7176422\n8\n1.515010\n17.32157\n\n\n0.9277365\n9\n1.860695\n18.84897\n\n\n0.2842301\n10\n1.080787\n17.58318\n\n\n\n\n\n\n\n\n2.2 R√©gression lin√©aire\nEffectuons la r√©gression lin√©aire de \\(Y\\) sur \\(X_1\\), \\(X_2\\) et \\(X_3\\) gr√¢ce √† la fonction lm() (linear model).\nElle va estimer les coefficients \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\) et \\(\\beta_3\\) ainsi que la qualit√© de l‚Äôestimation.\n\nR√©sultatCoefficientsStats globales\n\n\n\nreg &lt;- lm(Y ~ X1 + X2 + X3,\n          data = df)\n\nsummary(reg)\n\n\nCall:\nlm(formula = Y ~ X1 + X2 + X3, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.57715 -0.56368  0.04049  0.57072  2.87147 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 15.801475   0.863889  18.291  &lt; 2e-16 ***\nX1           4.840852   1.065220   4.544 9.62e-06 ***\nX2           0.102229   0.001189  85.998  &lt; 2e-16 ***\nX3          -0.994179   1.035373  -0.960    0.338    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9666 on 196 degrees of freedom\nMultiple R-squared:  0.9749,    Adjusted R-squared:  0.9745 \nF-statistic:  2536 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nLa fonction summary() fournit le tableau des coefficients avec :\n\nEstimate : les valeurs estim√©es des coefficients \\(\\beta\\)\nStd. Error : l‚Äôerreur standard associ√©e √† chaque coefficient\n\ncela donne une id√©e de la pr√©cision des estimations\n\nt value : test de Student (\\(H_0\\) : le coefficient est √©gal √† z√©ro, i.e.¬†la variable explicative n‚Äôa pas d‚Äôeffet sur \\(Y\\))\nPr(&gt;|t|) : La p-valeur associ√©e au test\n\nune p-valeur faible indique que le coefficient est statistiquement significatif\n\n*** : plus il y a d‚Äô√©toiles plus le coefficient est significatif\n\n\n\nLa fonction summary() fournit √©galement des statistiques globales du mod√®le.\n\nResidual standard error :\n\nl‚Äô√©cart-type des r√©sidus\nmesure la variabilit√© des valeurs de Y non expliqu√©es par le mod√®le\n\nMultiple R-squared : R¬≤ (Coefficient de d√©termination)\n\nindique la proportion de la variance de Y expliqu√©e par le mod√®le\nil varie entre 0 et 1, o√π 1 indique un ajustement parfait.\n\nF-statistic : test de Fischer de significativit√© du mod√®le.\n\n\\(H_0\\) : tous les coefficients de r√©gression sont √©gaux √† z√©ro (i.e.¬†les variables explicatives n‚Äôont aucun effet sur \\(Y\\))\n\n\n\n\n\n\n\n2.3 Application du mod√®le\nMaintenant que nous avons notre mod√®le, calculons et affichons :\n\nles valeurs ajust√©es \\(\\hat{Y}\\) (pr√©dites par le mod√®le)\nles r√©sidus \\(\\hat{\\epsilon}\\) (diff√©rence en la valeur r√©elle et la valeur pr√©dite)\n\n\ndf$Y_hat &lt;- reg$fitted.values               # ou df$Y_hat &lt;- predict(reg, subset(df, select = -c(Y)))\ndf$Epsilon_hat &lt;- reg$residuals             # ou df$Epsilon_hat &lt;- resid(reg) ou df$Epsilon_hat = df$Y - df$Y_hat\n\nhead(df, 10)\n\n\n\n\n\nX1\nX2\nX3\nY\nY_hat\nEpsilon_hat\n\n\n\n\n0.7456200\n1\n1.555949\n18.30523\n17.96625\n0.3389790\n\n\n0.7817124\n2\n1.611074\n19.07200\n18.18839\n0.8836105\n\n\n0.0200371\n3\n1.000401\n14.91214\n15.21058\n-0.2984365\n\n\n0.7760854\n4\n1.602308\n18.87371\n18.37432\n0.4993826\n\n\n0.0669101\n5\n1.004477\n13.75483\n15.63789\n-1.8830648\n\n\n0.6447951\n6\n1.415761\n17.99670\n18.12869\n-0.1319815\n\n\n0.9293860\n7\n1.863758\n19.79870\n19.16319\n0.6355146\n\n\n0.7176422\n8\n1.515010\n17.32157\n18.58711\n-1.2655409\n\n\n0.9277365\n9\n1.860695\n18.84897\n19.36271\n-0.5137369\n\n\n0.2842301\n10\n1.080787\n17.58318\n17.12518\n0.4579915\n\n\n\n\n\nggplot(df, aes(x = 1:nrow(df))) + \n  geom_point(aes(y = Y, color = \"Observ√©\"), size = 1) + \n  geom_point(aes(y = Y_hat, color = \"Ajust√©\"), shape = 18, size = 2) +\n  labs(x = \"Index\", y = \"Valeurs\", title = \"Valeurs Observ√©es et Ajust√©es\") +\n  scale_color_manual(values = c(\"Observ√©\" = \"darkcyan\", \"Ajust√©\" = \"orange\")) +\n  theme_minimal()"
  },
  {
    "objectID": "doc/tuto/stats/statistique-inferentielle.html",
    "href": "doc/tuto/stats/statistique-inferentielle.html",
    "title": "Statistique inf√©rentielle",
    "section": "",
    "text": "üöß"
  },
  {
    "objectID": "doc/tuto/stats/statistique-inferentielle.html#d√©finitions-et-notations",
    "href": "doc/tuto/stats/statistique-inferentielle.html#d√©finitions-et-notations",
    "title": "Statistique inf√©rentielle",
    "section": "1 D√©finitions et notations",
    "text": "1 D√©finitions et notations\n\nSoit \\(X_i\\) une variable al√©atoire (va). Notons son esp√©rance et sa variance :\n\n\\(\\mathbb{E}(X) = \\mu\\)\n\\(\\text{Var}(X) = \\sigma^2\\)\n\nSoit \\(x_i\\) une r√©alisation de cette va\nUne Statistique est une variable al√©atoire, fonction d‚Äôun √©chantillon\n\nnot√©e \\(T_n = h(X_1,...,X_n)\\)\navec \\(n\\) : taille de l'√©chantillon\n\n\nCommen√ßons par pr√©senter les 2 statistiques les plus connues : la moyenne et la variance empiriques."
  },
  {
    "objectID": "doc/tuto/stats/statistique-inferentielle.html#moyenne-empirique",
    "href": "doc/tuto/stats/statistique-inferentielle.html#moyenne-empirique",
    "title": "Statistique inf√©rentielle",
    "section": "2 Moyenne empirique",
    "text": "2 Moyenne empirique\n\\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\)\nOn prouve assez facilement que :\n\n\\(\\mathbb{E}(bar{X}) = \\mu\\)\n\\(\\text{Var}(\\bar{X}) = \\frac{\\sigma^2}{n}\\)"
  },
  {
    "objectID": "doc/tuto/stats/statistique-inferentielle.html#variance-empirique",
    "href": "doc/tuto/stats/statistique-inferentielle.html#variance-empirique",
    "title": "Statistique inf√©rentielle",
    "section": "3 Variance empirique",
    "text": "3 Variance empirique\n\\(\\tilde{S}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\bar{X})^2\\)\n\\(\\mathbb{E}[\\tilde{S}^2] = \\frac{n-1}{n} \\sigma^2\\)\n\n3.1 Variance empirique corrig√©e\n\\(S^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2\\)\n\\(\\mathbb{E}[S^2] = \\sigma^2\\)"
  },
  {
    "objectID": "doc/tuto/stats/modeles-survie.html",
    "href": "doc/tuto/stats/modeles-survie.html",
    "title": "Mod√®les de Survie",
    "section": "",
    "text": "R√©initialisation de l‚Äôenvironnement\n\nrm(list=ls())\n\nLibrairies utiles\n\nlibrary(dplyr)\nlibrary(survival)"
  },
  {
    "objectID": "doc/tuto/stats/modeles-survie.html#avant-de-commencer",
    "href": "doc/tuto/stats/modeles-survie.html#avant-de-commencer",
    "title": "Mod√®les de Survie",
    "section": "",
    "text": "R√©initialisation de l‚Äôenvironnement\n\nrm(list=ls())\n\nLibrairies utiles\n\nlibrary(dplyr)\nlibrary(survival)"
  },
  {
    "objectID": "doc/tuto/stats/modeles-survie.html#g√©n√©ration-des-donn√©es",
    "href": "doc/tuto/stats/modeles-survie.html#g√©n√©ration-des-donn√©es",
    "title": "Mod√®les de Survie",
    "section": "2 G√©n√©ration des donn√©es",
    "text": "2 G√©n√©ration des donn√©es\n\nDur√©es de vie : \\(X \\sim Exp(0.3)\\)\nCensures √† droite : \\(C \\sim Exp(0.1)\\)\nTemps d‚Äôobservation : \\(T = min(X,C)\\)\n\n\nset.seed(20)\nn &lt;- 150\n\nX &lt;- rexp(n, 0.3)               # Temps de survie (Xi)\nC &lt;- rexp(n, 0.1)               # Temps de censures √† droite (Ci)\nT &lt;- pmin(X, C)                 # Temps d'observation (Ti)\n\ndata &lt;- data.frame(X, C, T)\n\n# statut de censure (0 : censure √† droite, 1 : pas de censure, 2 : censure √† gauche)\ndata$censure &lt;- ifelse(X &lt;= C, 1, 0)\n\nhead(data, 10)\n\n\n\n\n\nX\nC\nT\ncensure\n\n\n\n\n0.6445417\n1.708531\n0.6445417\n1\n\n\n0.1944246\n3.126684\n0.1944246\n1\n\n\n0.2110231\n2.196058\n0.2110231\n1\n\n\n7.3714440\n10.960311\n7.3714440\n1\n\n\n3.3450766\n5.479652\n3.3450766\n1\n\n\n3.9114845\n1.356186\n1.3561858\n0\n\n\n1.4368504\n4.680558\n1.4368504\n1\n\n\n1.7186424\n5.867553\n1.7186424\n1\n\n\n21.2389967\n4.111656\n4.1116558\n0\n\n\n3.2724543\n34.235329\n3.2724543\n1"
  },
  {
    "objectID": "doc/tuto/stats/modeles-survie.html#mod√®les-param√©triques",
    "href": "doc/tuto/stats/modeles-survie.html#mod√®les-param√©triques",
    "title": "Mod√®les de Survie",
    "section": "3 Mod√®les param√©triques",
    "text": "3 Mod√®les param√©triques\nLes fonctions de risque instantan√© des deux principaux mod√®les :\n\nMod√®le exponentiel : \\(\\lambda(t,\\ Z)\\ =\\ \\theta\\ exp(\\beta^T Z)\\)\nMod√®le de Weibull : \\(\\lambda(t,\\ Z)\\ =\\ \\theta\\ \\gamma\\ (\\theta\\ t)^{\\gamma-1} exp(\\beta^T Z)\\)\n\navec \\(Z\\) : les covariables\n\n3.1 Estimation param√©trique du mod√®le sans prise en compte des censures\nOn suppose que toutes les observations sont vraies ie. les censures sont consid√©r√©es comme des d√©c√®s\n\nmodel1 &lt;- survreg(Surv(data$T, rep(1,n)) ~ 1, dist = \"exponential\")\nsummary(model1)\n\n\nCall:\nsurvreg(formula = Surv(data$T, rep(1, n)) ~ 1, dist = \"exponential\")\n             Value Std. Error    z      p\n(Intercept) 0.9466     0.0816 11.6 &lt;2e-16\n\nScale fixed at 1 \n\nExponential distribution\nLoglik(model)= -292   Loglik(intercept only)= -292\nNumber of Newton-Raphson Iterations: 5 \nn= 150 \n\nexp(-model1$coefficients)          # Valeur du param√®tre de la loi exponentielle estim√©e\n\n(Intercept) \n  0.3880494 \n\nexp(-model1$coefficients) - 0.3    # Biais\n\n(Intercept) \n 0.08804942 \n\n\n\n\n3.2 Estimation param√©trique du mod√®le en supprimant les censures\n\nmodel2 &lt;- survreg(Surv(data$T, data$censure) ~ 1, \n                  dist = \"exponential\",\n                  subset = data$censure == 1)\nsummary(model2)\n\n\nCall:\nsurvreg(formula = Surv(data$T, data$censure) ~ 1, subset = data$censure == \n    1, dist = \"exponential\")\n             Value Std. Error z      p\n(Intercept) 0.8622     0.0958 9 &lt;2e-16\n\nScale fixed at 1 \n\nExponential distribution\nLoglik(model)= -203   Loglik(intercept only)= -203\nNumber of Newton-Raphson Iterations: 5 \nn= 109 \n\nexp(-model2$coefficients)          # Valeur du param√®tre de la loi exponentielle estim√©e\n\n(Intercept) \n  0.4222224 \n\nexp(-model2$coefficients) - 0.3    # Biais\n\n(Intercept) \n  0.1222224 \n\n\n\n\n3.3 Estimation param√©trique du mod√®le en consid√©rant les censures\n\nmodel3 &lt;- survreg(Surv(data$T, data$censure) ~ 1, \n                  dist = \"exponential\")\nsummary(model3)\n\n\nCall:\nsurvreg(formula = Surv(data$T, data$censure) ~ 1, dist = \"exponential\")\n             Value Std. Error    z      p\n(Intercept) 1.2659     0.0958 13.2 &lt;2e-16\n\nScale fixed at 1 \n\nExponential distribution\nLoglik(model)= -247   Loglik(intercept only)= -247\nNumber of Newton-Raphson Iterations: 5 \nn= 150 \n\nexp(-model3$coefficients)          # Valeur du param√®tre de la loi exponentielle estim√©e\n\n(Intercept) \n  0.2819826 \n\nexp(-model3$coefficients) - 0.3    # Biais\n\n(Intercept) \n-0.01801742 \n\n\n\n\n3.4 Comparaisons\n\ndata_compare &lt;- data %&gt;%\n  arrange(T) %&gt;%\n  mutate(no_censor = exp(-exp(-model1$coefficients) * T),\n         tronc_censor = ifelse(censure == 1, exp(-exp(-model2$coefficients) * T), NA),\n         with_censor = exp(-exp(-model3$coefficients) * T))\n\n\nplot(with_censor ~ T, data = data_compare, \n     type = \"l\", col = \"green\", \n     xlab = \"Probabilit√© de Survie\", ylab = \"Temps\")\nlines(no_censor ~ T, data = data_compare, type = \"l\", col = \"orange\")\nlines(tronc_censor ~ T, data = data_compare, type = \"l\", col = \"red\")\nlegend(\"topright\", \n       legend = c(\"Mod√®le param√©trique avec censures\", \n                \"Mod√®le param√©trique sans censures\", \n                \"Mod√®le param√©trique avec donn√©es tronqu√©es\"),\n       col=c(\"green\", \"orange\", \"red\"), \n       lty = 1:1)"
  },
  {
    "objectID": "doc/tuto/stats/modeles-survie.html#mod√®les-non-param√©triques",
    "href": "doc/tuto/stats/modeles-survie.html#mod√®les-non-param√©triques",
    "title": "Mod√®les de Survie",
    "section": "4 Mod√®les non param√©triques",
    "text": "4 Mod√®les non param√©triques\n\n4.1 Courbe de survie par la m√©thode de Kaplan-Meier\n\nKaplan_Meier &lt;- survfit(Surv(data$T, data$censure) ~ 1,\n                        type = \"kaplan-meier\",\n                        error = c(\"greenwood\"))\n\nKaplan_Meier\n\nCall: survfit(formula = Surv(data$T, data$censure) ~ 1, error = c(\"greenwood\"), \n    type = \"kaplan-meier\")\n\n       n events median 0.95LCL 0.95UCL\n[1,] 150    109   2.38    1.71    3.13\n\n\nCalcul pas √† pas de la fonction de Survie, √† l‚Äôinstant t :\n\nrisque = nombre de d√©c√®s / nombre d‚Äôindividus √† risque\nsurvie = produit de (1 - risque) pour tous les instants inf√©rieurs ou √©gal √† t\n\n\ntaleau_KM &lt;- data.frame(temps_obs = Kaplan_Meier$time,\n                       nb_deces = Kaplan_Meier$n.event,\n                       nb_censures = Kaplan_Meier$n.censor,\n                       nb_a_risque = Kaplan_Meier$n.risk)\n\ntaleau_KM &lt;- taleau_KM %&gt;%\n  mutate(risque = nb_deces / nb_a_risque) %&gt;%\n  mutate(survie = cumprod(1 - risque))             # = Kaplan_Meier$surv\n\nhead(taleau_KM, 10)\n\n\n\n\n\ntemps_obs\nnb_deces\nnb_censures\nnb_a_risque\nrisque\nsurvie\n\n\n\n\n0.0018103\n1\n0\n150\n0.0066667\n0.9933333\n\n\n0.0237061\n1\n0\n149\n0.0067114\n0.9866667\n\n\n0.0483489\n1\n0\n148\n0.0067568\n0.9800000\n\n\n0.0487304\n1\n0\n147\n0.0068027\n0.9733333\n\n\n0.0566822\n1\n0\n146\n0.0068493\n0.9666667\n\n\n0.0576510\n0\n1\n145\n0.0000000\n0.9666667\n\n\n0.0690159\n0\n1\n144\n0.0000000\n0.9666667\n\n\n0.0705573\n1\n0\n143\n0.0069930\n0.9599068\n\n\n0.0727950\n1\n0\n142\n0.0070423\n0.9531469\n\n\n0.0750514\n1\n0\n141\n0.0070922\n0.9463869\n\n\n\n\n\ntail(taleau_KM, 10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntemps_obs\nnb_deces\nnb_censures\nnb_a_risque\nrisque\nsurvie\n\n\n\n\n141\n7.599810\n1\n0\n10\n0.1000000\n0.1262141\n\n\n142\n7.893794\n0\n1\n9\n0.0000000\n0.1262141\n\n\n143\n7.904069\n1\n0\n8\n0.1250000\n0.1104373\n\n\n144\n7.932746\n0\n1\n7\n0.0000000\n0.1104373\n\n\n145\n8.870664\n1\n0\n6\n0.1666667\n0.0920311\n\n\n146\n8.930153\n1\n0\n5\n0.2000000\n0.0736249\n\n\n147\n9.880971\n1\n0\n4\n0.2500000\n0.0552187\n\n\n148\n10.243681\n1\n0\n3\n0.3333333\n0.0368124\n\n\n149\n11.783160\n0\n1\n2\n0.0000000\n0.0368124\n\n\n150\n13.341401\n0\n1\n1\n0.0000000\n0.0368124\n\n\n\n\n\n\n\nplot(with_censor ~ T, data = data_compare, \n     type = \"l\", col = \"green\", \n     xlab = \"Probabilit√© de Survie\", ylab = \"Temps\")\n#lines(KM ~ T, data = data_compare, type = \"l\", col = \"blue\")\nlines(Kaplan_Meier, col = \"blue\")\nlegend(\"topright\", \n       legend = c(\"Mod√®le param√©trique avec censures\", \n                \"Courbe de Kaplan-Meier\"),\n       col=c(\"green\", \"blue\"), \n       lty = 1:1)\n\n\n\n\n\n\n\n\n\n\n4.2 Courbe de survie par m√©thode actuarielle\nüöß\n\n\n4.3 Comparaisons\n\n# Calcul des fonctions de survie\n# S(t) = exp(-lambda) = exp(-exp(fit_event$coefficients))\n\ndata_compare &lt;- data_compare %&gt;%\n  arrange(T) %&gt;%\n  mutate(KM = Kaplan_Meier$surv)\n\n\nplot(with_censor ~ T, data = data_compare, \n     type = \"l\", col = \"green\", \n     xlab = \"Probabilit√© de Survie\", ylab = \"Temps\")\nlines(KM ~ T, data = data_compare, type = \"l\", col = \"blue\")\nlegend(\"topright\", \n       legend = c(\"Mod√®le param√©trique avec censures\", \n                \"Courbe de Kaplan-Meier\"),\n       col=c(\"green\", \"blue\"), \n       lty = 1:1)"
  },
  {
    "objectID": "doc/tuto/stats/modeles-survie.html#mod√®les-semi-param√©triques",
    "href": "doc/tuto/stats/modeles-survie.html#mod√®les-semi-param√©triques",
    "title": "Mod√®les de Survie",
    "section": "5 Mod√®les semi-param√©triques",
    "text": "5 Mod√®les semi-param√©triques\n\n5.1 Mod√®le de Cox\n\\(\\lambda(t,\\ Z)\\ =\\ \\lambda_0(t)\\ exp(\\beta^T Z)\\) avec \\(\\lambda_0(t)\\) non sp√©cifi√©\n\ndata7 &lt;- data.frame(sexe = rbinom(n, 1, prob = 0.5),\n                    age = round(runif(n, 20, 80)),\n                    dose = rpois(n, 5))\n\ndata7 &lt;- data7 %&gt;%\n  mutate(X = 100 * rexp(n, 0.1 + age/10 + sexe),\n         C = 100 *rexp(n, 0.1)) %&gt;%\n  mutate(T = pmin(X, C),\n         censure = ifelse(X &lt;= C, 1, 0))\n\n\nplot(data7$age, data7$X, xlab = \"Age\", ylab = \"Temps de survie\", pch = 3, col = data7$sexe + 1)\nlegend(\"topright\", \n       legend=c(\"Sexe = 0\", \"Sexe = 1\"),\n       col=c(\"black\", \"red\"), lty=1:1)\n\n\n\n\n\n\n\n\n\ncox_model &lt;- coxph(Surv(T, censure) ~ factor(sexe) + age + dose, \n                   data = data7)\nsummary(cox_model)\n\nCall:\ncoxph(formula = Surv(T, censure) ~ factor(sexe) + age + dose, \n    data = data7)\n\n  n= 150, number of events= 147 \n\n                   coef exp(coef)  se(coef)      z Pr(&gt;|z|)  \nfactor(sexe)1  0.202349  1.224275  0.172223  1.175   0.2400  \nage            0.010063  1.010114  0.004726  2.129   0.0332 *\ndose          -0.036768  0.963900  0.040850 -0.900   0.3681  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n              exp(coef) exp(-coef) lower .95 upper .95\nfactor(sexe)1    1.2243     0.8168    0.8735     1.716\nage              1.0101     0.9900    1.0008     1.020\ndose             0.9639     1.0375    0.8897     1.044\n\nConcordance= 0.571  (se = 0.026 )\nLikelihood ratio test= 7.22  on 3 df,   p=0.07\nWald test            = 7.33  on 3 df,   p=0.06\nScore (logrank) test = 7.37  on 3 df,   p=0.06\n\n\n\nplot(survfit(cox_model),\n     ylim = c(0,1),\n     col = \"green\",\n     xlab = \"Temps (jours)\",\n     ylab = \"Fonction de survie\",\n     main = \"Fonction de survie\")\n\nlines(survfit(Surv(T, censure) ~ 1, data = data7), col = \"blue\", conf.int = FALSE)\nlines(survfit(cox_model, newdata = data.frame(sexe = 0, dose = 5, age = 35)), col = \"orange\", conf.int = FALSE)\nlegend(\"topright\", \n       legend=c(\"Cox moyen\", \"Kaplan-Meier\", \"Cox ajust√© (sexe=0, age=35, dose = 5)\"),\n       col=c(\"green\", \"blue\",\"orange\"), lty=1:1)\n\n\n\n\n\n\n\n\n\n\n5.2 Comparaison de mod√®les avec un test de rapport de vraisemblance\nLog vraisemblance mod√®le non contraint\n\nloglik_M1 &lt;- cox_model$loglik[2]\n\nLog vraisemblance mod√®le contraint (non prise en compte de dose)\n\ncox_contraint &lt;- coxph(Surv(T, censure) ~ factor(sexe) + age, \n                   data = data7)\nsummary(cox_contraint)\n\nCall:\ncoxph(formula = Surv(T, censure) ~ factor(sexe) + age, data = data7)\n\n  n= 150, number of events= 147 \n\n                  coef exp(coef) se(coef)     z Pr(&gt;|z|)  \nfactor(sexe)1 0.191454  1.211009 0.171542 1.116   0.2644  \nage           0.010047  1.010098 0.004719 2.129   0.0332 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n              exp(coef) exp(-coef) lower .95 upper .95\nfactor(sexe)1     1.211     0.8258    0.8652     1.695\nage               1.010     0.9900    1.0008     1.019\n\nConcordance= 0.557  (se = 0.028 )\nLikelihood ratio test= 6.4  on 2 df,   p=0.04\nWald test            = 6.46  on 2 df,   p=0.04\nScore (logrank) test = 6.51  on 2 df,   p=0.04\n\n# Log vraisemblance mod√®le contraint\nloglik_M0 &lt;- cox_contraint$loglik[2]\n\nTest du rapport de vraisemblance\n\n\\(H_0\\) : le coefficient associ√© √† la covariable dose vaut 0\nSous \\(H_0\\), le rapport de vraisemblance suit une loi du chi2 √† m degr√©s de libert√© (o√π m est le nombre de param√®tres de diff√©rence entre les deux mod√®les)\n\n\nLRT &lt;- 2 * (loglik_M1 - loglik_M0)\n1 - pchisq(LRT, 1)\n\n[1] 0.365318\n\n\nLa p-valeur est tr√®s sup√©rieure √† 0.05, donc on ne rejette pas \\(H_0\\)."
  }
]